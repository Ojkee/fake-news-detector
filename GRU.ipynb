{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dec587a7-9d76-4d55-aea2-f91c1c098407",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa8f8d6b-aebd-4d9b-b28e-2fd5369e1c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataFrames:\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_file_name: str,\n",
    "        test_file_name: str,\n",
    "        evaluation_file_name: str,\n",
    "    ) -> None:\n",
    "        self.content_col_name: str = \"content\"\n",
    "        self.content_clean_col_name: str = \"content_clean\"\n",
    "        self.content_clean_col_name_no_UNK: str = \"content_clean_no_UNK\"\n",
    "        self.content_clean_col_name_truc: str = \"content_clean_truc\"\n",
    "        self.label_col_name: str = \"label\"\n",
    "        self.train: pd.DataFrame = self.load_data(train_file_name)\n",
    "        self.test: pd.DataFrame = self.load_data(test_file_name)\n",
    "        self.evaluation: pd.DataFrame = self.load_data(evaluation_file_name)\n",
    "\n",
    "    def load_data(self, file_name: str) -> pd.DataFrame:\n",
    "        df = pd.read_csv(f\"dataset/{file_name}.csv\", sep=\";\")\n",
    "        df.dropna(inplace=True)\n",
    "        if not self.content_clean_col_name in df.columns:\n",
    "            df[self.content_col_name] = df[\"title\"] + \" \" + df[\"text\"]\n",
    "            df = df.drop(columns=[\"Unnamed: 0\"])\n",
    "        return df\n",
    "\n",
    "    def get_datasets(self) -> list[pd.DataFrame]:\n",
    "        return [self.train, self.test, self.evaluation]\n",
    "\n",
    "    def get_info(self) -> str:\n",
    "        test_info = self.test.shape\n",
    "        train_info = self.train.shape\n",
    "        evaluation_info = self.evaluation.shape\n",
    "        return f\"DataFrame Shapes:\\n\\tTrain: {train_info}\\n\\tTest: {test_info}\\n\\tEvaluation: {evaluation_info}\\n\"\n",
    "\n",
    "    def save_clean(self, token_limit: int = 10000, num_words_trunc: int = 256) -> None:\n",
    "        # CLEAN\n",
    "        self._init_clean_content([self.train, self.test, self.evaluation])\n",
    "        self.train = self.clean_df(self.train)\n",
    "        self.test = self.clean_df(self.test)\n",
    "        self.evaluation = self.clean_df(self.evaluation)\n",
    "\n",
    "        most_common_words = self.get_most_common_words_counter(\n",
    "            [self.test, self.train],\n",
    "            token_limit,\n",
    "            self.content_clean_col_name,\n",
    "        )\n",
    "        self.train = DataFrames.set_least_common_UNK(\n",
    "            self.train, \"content_clean\", most_common_words\n",
    "        )\n",
    "        self.test = DataFrames.set_least_common_UNK(\n",
    "            self.test, \"content_clean\", most_common_words\n",
    "        )\n",
    "        self.evaluation = DataFrames.set_least_common_UNK(\n",
    "            self.evaluation, \"content_clean\", most_common_words\n",
    "        )\n",
    "        self.train = DataFrames.drop_least_common(\n",
    "            self.train,\n",
    "            self.content_clean_col_name,\n",
    "            self.content_clean_col_name_no_UNK,\n",
    "            most_common_words,\n",
    "        )\n",
    "        self.test = DataFrames.drop_least_common(\n",
    "            self.test,\n",
    "            self.content_clean_col_name,\n",
    "            self.content_clean_col_name_no_UNK,\n",
    "            most_common_words,\n",
    "        )\n",
    "        self.evaluation = DataFrames.drop_least_common(\n",
    "            self.evaluation,\n",
    "            self.content_clean_col_name,\n",
    "            self.content_clean_col_name_no_UNK,\n",
    "            most_common_words,\n",
    "        )\n",
    "        self.train = DataFrames.trunc_text(\n",
    "            self.train,\n",
    "            self.content_clean_col_name_no_UNK,\n",
    "            self.content_clean_col_name_truc,\n",
    "            num_words_trunc,\n",
    "        )\n",
    "        self.test = DataFrames.trunc_text(\n",
    "            self.test,\n",
    "            self.content_clean_col_name_no_UNK,\n",
    "            self.content_clean_col_name_truc,\n",
    "            num_words_trunc,\n",
    "        )\n",
    "        self.evaluation = DataFrames.trunc_text(\n",
    "            self.evaluation,\n",
    "            self.content_clean_col_name_no_UNK,\n",
    "            self.content_clean_col_name_truc,\n",
    "            num_words_trunc,\n",
    "        )\n",
    "        print(self.test.columns)\n",
    "        # SAVE\n",
    "        cols_to_save_clean: list[str] = [\n",
    "            self.content_clean_col_name,\n",
    "            self.content_clean_col_name_no_UNK,\n",
    "            self.content_clean_col_name_truc,\n",
    "            self.label_col_name,\n",
    "        ]\n",
    "        self.train.to_csv(\n",
    "            f\"dataset/train_clean.csv\",\n",
    "            sep=\";\",\n",
    "            columns=cols_to_save_clean,\n",
    "        )\n",
    "        self.test.to_csv(\n",
    "            f\"dataset/test_clean.csv\",\n",
    "            sep=\";\",\n",
    "            columns=cols_to_save_clean,\n",
    "        )\n",
    "        self.evaluation.to_csv(\n",
    "            f\"dataset/evaluation_clean.csv\",\n",
    "            sep=\";\",\n",
    "            columns=cols_to_save_clean,\n",
    "        )\n",
    "\n",
    "    def num_unique_words(self, col_name: str) -> int:\n",
    "        result: set = set()\n",
    "        df = pd.concat([self.train, self.test, self.evaluation], ignore_index=True)\n",
    "        df[col_name].str.lower().str.split().apply(result.update)\n",
    "        return len(result)\n",
    "\n",
    "    def get_vocab(self, col_name) -> dict[str, int]:\n",
    "        vectorizer = CountVectorizer()\n",
    "        for df in [self.train, self.test, self.evaluation]:\n",
    "            vectorizer.fit_transform(df[col_name].values)\n",
    "        return vectorizer.vocabulary_\n",
    "\n",
    "    def _init_clean_content(self, dfs: list[pd.DataFrame]) -> list[pd.DataFrame]:\n",
    "        for df in dfs:\n",
    "            df[self.content_clean_col_name] = df[self.content_col_name]\n",
    "        return dfs\n",
    "\n",
    "    def clean_df(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        df = self.to_lower(df, self.content_clean_col_name)\n",
    "        df = self.remove_punctuation(df, self.content_clean_col_name)\n",
    "        df = self.remove_stopword(df, self.content_clean_col_name)\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def to_lower(df: pd.DataFrame, col_name: str) -> pd.DataFrame:\n",
    "        df[col_name] = df[col_name].apply(lambda x: str(x).lower())\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_punctuation(df: pd.DataFrame, col_name: str) -> pd.DataFrame:\n",
    "        re_punctuation = f'[{re.escape(string.punctuation)}\"”“]'\n",
    "        df[col_name] = df[col_name].apply(\n",
    "            lambda x: re.sub(re_punctuation, \" \", str(x))\n",
    "            .lower()\n",
    "            .replace(\"'s\", \"\")\n",
    "            .replace(\"’s\", \"\")\n",
    "        )\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_stopword(df: pd.DataFrame, col_name: str) -> pd.DataFrame:\n",
    "        stop_words = set(stopwords.words(\"english\"))\n",
    "        df[col_name] = df[col_name].apply(\n",
    "            lambda x: \" \".join(\n",
    "                word for word in str(x).split() if not word in stop_words\n",
    "            )\n",
    "        )\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def get_most_common_words_counter(\n",
    "        dfs: list[pd.DataFrame],\n",
    "        token_limit: int,\n",
    "        col_name: str,\n",
    "    ) -> Counter:\n",
    "        word_counter: Counter = Counter()\n",
    "        for df in dfs:\n",
    "            if col_name not in df.columns:\n",
    "                raise ValueError(\"Each DataFrame must have a 'clean_content' column\")\n",
    "            tokens = \" \".join(df[col_name].astype(str)).split()\n",
    "            word_counter.update(tokens)\n",
    "        return Counter(dict(word_counter.most_common(token_limit)))\n",
    "\n",
    "    @staticmethod\n",
    "    def set_least_common_UNK(\n",
    "        df: pd.DataFrame,\n",
    "        col_name: str,\n",
    "        most_common_words: Counter,\n",
    "    ) -> pd.DataFrame:\n",
    "        df[col_name] = df[col_name].apply(\n",
    "            lambda x: \" \".join(\n",
    "                [\n",
    "                    word if word in most_common_words else \"<UNK>\"\n",
    "                    for word in str(x).split()\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def drop_least_common(\n",
    "        df: pd.DataFrame,\n",
    "        col_name: str,\n",
    "        col_name_no_unk: str,\n",
    "        most_common_words: Counter,\n",
    "    ) -> pd.DataFrame:\n",
    "        df[col_name_no_unk] = df[col_name].apply(\n",
    "            lambda x: \" \".join(\n",
    "                [word for word in str(x).split() if word in most_common_words]\n",
    "            )\n",
    "        )\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def trunc_text(\n",
    "        df: pd.DataFrame,\n",
    "        col_name: str,\n",
    "        col_name_trunc: str,\n",
    "        trunc_num: int,\n",
    "    ) -> pd.DataFrame:\n",
    "        df[col_name_trunc] = df[col_name].apply(\n",
    "            lambda x: \" \".join(str(x).split()[:trunc_num])\n",
    "        )\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def label_to_str(label: int) -> str:\n",
    "        return \"Fake\" if label == 1 else \"Not Fake\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53e50957-d64b-4e86-a853-cdf79e8ab335",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import (\n",
    "    pack_padded_sequence,\n",
    "    pad_sequence,\n",
    "    pad_packed_sequence,\n",
    "    PackedSequence,\n",
    ")\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9dae5130-5cf8-4349-8905-3171bf9fb272",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUClassifier(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        embedding_dim: int,\n",
    "        hidden_dim: int,\n",
    "        num_layers: int,\n",
    "        dropout: float,\n",
    "        lr: float,\n",
    "        *kwargs,\n",
    "        **args,\n",
    "    ) -> None:\n",
    "        super().__init__(*kwargs, **args)\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim).to(self.device)\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0.0,\n",
    "        ).to(self.device)\n",
    "        self.fc = nn.Linear(hidden_dim, 1).to(self.device)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.criterion = nn.BCEWithLogitsLoss()\n",
    "        self.optimizer = optim.Adam(\n",
    "            list(self.embedding.parameters())\n",
    "            + list(self.gru.parameters())\n",
    "            + list(self.fc.parameters()),\n",
    "            lr=lr,\n",
    "        )\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return (\n",
    "            f\"GRUClassifier:\\n\"\n",
    "            f\"Vocab size: {self.embedding.num_embeddings}, \"\n",
    "            f\"Embedding dim: {self.embedding.embedding_dim}, \"\n",
    "            f\"Hidden dim: {self.gru.hidden_size}, \"\n",
    "            f\"Layers: {self.gru.num_layers}, \"\n",
    "        )\n",
    "\n",
    "    def print_info(self) -> None:\n",
    "        print(self)\n",
    "\n",
    "    def _pass(self, packed_input: PackedSequence) -> torch.Tensor:\n",
    "        packed_output, _ = self.gru(packed_input)\n",
    "        gru_output, _ = pad_packed_sequence(packed_output, batch_first=True)\n",
    "        output = self.fc(gru_output)\n",
    "        output = self.sigmoid(output)\n",
    "        return output\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        X: PackedSequence,\n",
    "        y: torch.Tensor,\n",
    "        num_epoch: int,\n",
    "        batch_size: int,\n",
    "    ) -> float:\n",
    "        self.embedding.train()\n",
    "        self.gru.train()\n",
    "        total_loss: float = 0\n",
    "\n",
    "        for epoch in range(num_epoch):\n",
    "            running_loss = 0.0\n",
    "            for i in range(0, len(X), batch_size):\n",
    "                batch_X = X[i : i + batch_size]\n",
    "                batch_y = y[i : i + batch_size]\n",
    "                if batch_X[0] is None or batch_X[1] is None:\n",
    "                    continue\n",
    "                batch_X = PackedSequence(\n",
    "                    batch_X[0].to(self.device),\n",
    "                    batch_X[1],\n",
    "                )\n",
    "                batch_y = batch_y.to(self.device)\n",
    "                self.optimizer.zero_grad()\n",
    "                output = self._pass(batch_X)\n",
    "                loss = self.criterion(output.squeeze(), batch_y.float())\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "            total_loss += running_loss / len(X)\n",
    "            print(f\"Epoch [{epoch+1}/{num_epoch}], Loss: {running_loss/len(X)}\")\n",
    "        return total_loss / num_epoch\n",
    "\n",
    "    def predict(self, X: PackedSequence):\n",
    "        self.embedding.eval()\n",
    "        self.gru.eval()\n",
    "        self.fc.eval()\n",
    "        with torch.no_grad():\n",
    "            X = X.to(self.device)\n",
    "            gru_output, _ = self.gru(X)\n",
    "            logits = self.fc(gru_output[:, -1, :]).squeeze(1)\n",
    "            return torch.sigmoid(logits).cpu().numpy()\n",
    "\n",
    "    def print_report(self, y_test, y_hat) -> None:\n",
    "        report = classification_report(y_test, y_hat, target_names=[\"Not Fake\", \"Fake\"])\n",
    "        print(f\"{self.__class__.__name__} Report:\\n{report}\")\n",
    "\n",
    "    def get_embedded(self, X, y, vocab: dict[str, int]):\n",
    "        self.embedding.eval()\n",
    "        all_embeddings = []\n",
    "        valid_indices = []\n",
    "        valid_labels = []\n",
    "        for idx, text in enumerate(X):\n",
    "            if text.strip():\n",
    "                words = text.split()\n",
    "                indices = [vocab.get(word, -1) for word in words]\n",
    "                indices = list(filter(lambda x: x != -1, indices))\n",
    "                if len(indices) > 0:\n",
    "                    indices_tensor = torch.tensor(indices, dtype=torch.long).to(\n",
    "                        self.device\n",
    "                    )\n",
    "                    text_embedding = self.embedding(indices_tensor)\n",
    "                    all_embeddings.append(text_embedding)\n",
    "                    valid_indices.append(idx)\n",
    "                    valid_labels.append(y[idx])\n",
    "        lengths = torch.tensor([seq.size(0) for seq in all_embeddings])\n",
    "        sorted_lengths, sorted_idx = lengths.sort(0, descending=True)\n",
    "        sorted_sequences = [all_embeddings[i] for i in sorted_idx]\n",
    "        padded_sequences = pad_sequence(sorted_sequences, batch_first=True).to(\n",
    "            self.device\n",
    "        )\n",
    "        packed_input = pack_padded_sequence(\n",
    "            padded_sequences,\n",
    "            sorted_lengths,\n",
    "            batch_first=True,\n",
    "            enforce_sorted=True,\n",
    "        )\n",
    "        return packed_input.cpu(), torch.tensor(valid_labels).cpu()\n",
    "\n",
    "\n",
    "def print_alocated_gpu_mem():\n",
    "    allocated_memory = torch.cuda.memory_allocated()\n",
    "    cached_memory = torch.cuda.memory_reserved()\n",
    "    allocated_memory_gb = allocated_memory / (1024**3)\n",
    "    cached_memory_gb = cached_memory / (1024**3)\n",
    "    print(f\"Memory allocated: {allocated_memory_gb:.2f} GB\")\n",
    "    print(f\"Memory cached: {cached_memory_gb:.2f} GB\")\n",
    "\n",
    "\n",
    "def run_gru(\n",
    "    data_frames: DataFrames,\n",
    "    content_col_name: str,\n",
    "    label_col_name: str,\n",
    "    vocab_size: int,\n",
    "    embedding_dim: int = 8,\n",
    "    hidden_dim: int = 64,\n",
    "    num_layers: int = 1,\n",
    "    dropout: float = 0.25,\n",
    "    lr: float = 0.001,\n",
    "    epochs: int = 2,\n",
    "    batch_size: int = 2,\n",
    ") -> float:\n",
    "    gru = GRUClassifier(\n",
    "        vocab_size=vocab_size,\n",
    "        embedding_dim=embedding_dim,\n",
    "        hidden_dim=hidden_dim,\n",
    "        num_layers=num_layers,\n",
    "        dropout=dropout,\n",
    "        lr=lr,\n",
    "    )\n",
    "    print_alocated_gpu_mem()\n",
    "\n",
    "    print(\"LOADING VOCAB AND PREPARE DATA\")\n",
    "    vocab = data_frames.get_vocab(content_col_name)\n",
    "    X_embedded, y_tenstor = gru.get_embedded(\n",
    "        data_frames.train[content_col_name].values,\n",
    "        data_frames.train[label_col_name].values,\n",
    "        vocab,\n",
    "    )\n",
    "    print(\"LOADING VOCAB AND PREPARE DATA: FINISHED\")\n",
    "    print_alocated_gpu_mem()\n",
    "\n",
    "    print(\"START TRAIN\")\n",
    "    # TRAIN\n",
    "    start_time_gru = time.time()\n",
    "    gru.forward(\n",
    "        X_embedded,\n",
    "        y_tenstor,\n",
    "        epochs,\n",
    "        batch_size,\n",
    "    )\n",
    "    end_time_gru = time.time()\n",
    "    print(\"END TRAIN\")\n",
    "    del X_embedded, y_tenstor\n",
    "\n",
    "    # TEST\n",
    "    X_embedded_test, y_tenstor_test = gru.get_embedded(\n",
    "        data_frames.test[content_col_name].values,\n",
    "        data_frames.test[label_col_name].values,\n",
    "        vocab,\n",
    "    )\n",
    "    predictions = gru.predict(X_embedded_test)\n",
    "    gru.print_report(y_tenstor_test, predictions)\n",
    "\n",
    "    # EVAL\n",
    "    rand_sample = data_frames.evaluation.sample(1)\n",
    "    X_embedded_eval, _ = gru.get_embedded(\n",
    "        rand_sample[content_col_name].values,\n",
    "        rand_sample[label_col_name].values,\n",
    "        vocab,\n",
    "    )\n",
    "    pred_sample = gru.predict(X_embedded_eval)\n",
    "    label_str_true: str = data_frames.label_to_str(pred_sample[0])\n",
    "    label_str_predicted: str = data_frames.label_to_str(pred_sample[0])\n",
    "    print(f\"is:        {label_str_true}\\npredicted: {label_str_predicted}\")\n",
    "    return end_time_gru - start_time_gru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9f2809d-ed0b-47bc-bd54-548c8a4ea346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words dataframes: 394396\n",
      "Unique words dataframes clean: 1001\n"
     ]
    }
   ],
   "source": [
    "# DATA\n",
    "data_frames = DataFrames(\"train\", \"test\", \"evaluation\")\n",
    "data_frames_unique = data_frames.num_unique_words(\"content\")\n",
    "print(f\"Unique words dataframes: {data_frames_unique}\")\n",
    "# data_frames.save_clean(token_limit=1000)\n",
    "\n",
    "data_frames_clean = DataFrames(\"train_clean\", \"test_clean\", \"evaluation_clean\")\n",
    "data_frames_clean_unique = data_frames_clean.num_unique_words(\"content_clean\")\n",
    "print(f\"Unique words dataframes clean: {data_frames_clean_unique}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "873f6fbf-198d-4757-be81-3087010469f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTS WITH TIME\n",
    "times: dict[str, float] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2877bcf-30e5-4f2f-ae75-11dcf5b610f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory allocated: 0.00 GB\n",
      "Memory cached: 0.00 GB\n",
      "LOADING VOCAB AND PREPARE DATA\n",
      "LOADING VOCAB AND PREPARE DATA: FINISHED\n",
      "Memory allocated: 0.03 GB\n",
      "Memory cached: 0.49 GB\n",
      "START TRAIN\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 4.32 GiB. GPU 0 has a total capacity of 7.92 GiB of which 3.93 GiB is free. Including non-PyTorch memory, this process has 3.43 GiB memory in use. Of the allocated memory 3.04 GiB is allocated by PyTorch, and 281.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[0;32m----> 2\u001b[0m gru_time_clean \u001b[38;5;241m=\u001b[39m \u001b[43mrun_gru\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_frames_clean\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_frames_clean\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent_clean_col_name_truc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_frames_clean\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_col_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_frames_clean_unique\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m times[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGRU Clean\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m gru_time_clean\n\u001b[1;32m      9\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n",
      "Cell \u001b[0;32mIn[5], line 177\u001b[0m, in \u001b[0;36mrun_gru\u001b[0;34m(data_frames, content_col_name, label_col_name, vocab_size, embedding_dim, hidden_dim, num_layers, dropout, lr, epochs, batch_size)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;66;03m# TRAIN\u001b[39;00m\n\u001b[1;32m    176\u001b[0m start_time_gru \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 177\u001b[0m \u001b[43mgru\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_embedded\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_tenstor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    183\u001b[0m end_time_gru \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEND TRAIN\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[5], line 76\u001b[0m, in \u001b[0;36mGRUClassifier.forward\u001b[0;34m(self, X, y, num_epoch, batch_size)\u001b[0m\n\u001b[1;32m     74\u001b[0m batch_y \u001b[38;5;241m=\u001b[39m batch_y\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 76\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_X\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion(output\u001b[38;5;241m.\u001b[39msqueeze(), batch_y\u001b[38;5;241m.\u001b[39mfloat())\n\u001b[1;32m     78\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "Cell \u001b[0;32mIn[5], line 46\u001b[0m, in \u001b[0;36mGRUClassifier._pass\u001b[0;34m(self, packed_input)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_pass\u001b[39m(\u001b[38;5;28mself\u001b[39m, packed_input: PackedSequence) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m---> 46\u001b[0m     packed_output, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgru\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpacked_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m     gru_output, _ \u001b[38;5;241m=\u001b[39m pad_packed_sequence(packed_output, batch_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     48\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(gru_output)\n",
      "File \u001b[0;32m~/programming/fake-news-detector/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/programming/fake-news-detector/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/programming/fake-news-detector/lib/python3.12/site-packages/torch/nn/modules/rnn.py:1404\u001b[0m, in \u001b[0;36mGRU.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m   1392\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mgru(\n\u001b[1;32m   1393\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   1394\u001b[0m         hx,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1401\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first,\n\u001b[1;32m   1402\u001b[0m     )\n\u001b[1;32m   1403\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1404\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgru\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1405\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1406\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1408\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1409\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1410\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1411\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1412\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1413\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1414\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1415\u001b[0m output \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1416\u001b[0m hidden \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 4.32 GiB. GPU 0 has a total capacity of 7.92 GiB of which 3.93 GiB is free. Including non-PyTorch memory, this process has 3.43 GiB memory in use. Of the allocated memory 3.04 GiB is allocated by PyTorch, and 281.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gru_time_clean = run_gru(\n",
    "    data_frames_clean,\n",
    "    data_frames_clean.content_clean_col_name_truc,\n",
    "    data_frames_clean.label_col_name,\n",
    "    data_frames_clean_unique,\n",
    ")\n",
    "times[\"GRU Clean\"] = gru_time_clean\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "for fname, ftime in times.items():\n",
    "    print(f\"{fname}: {ftime:.5f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62da7684-04dc-4f95-9c87-9a6b2ca0a4b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Fake News Env)",
   "language": "python",
   "name": "fake-news-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
