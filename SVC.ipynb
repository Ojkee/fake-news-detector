{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4100543d-e5b9-4731-be4c-225c06d966bb",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e216275c-7e6c-4525-9044-70012752e72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2d3d86de-40ef-4c6f-a6dc-7d3d27d4296f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataFrames:\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_file_name: str,\n",
    "        test_file_name: str,\n",
    "        evaluation_file_name: str,\n",
    "    ) -> None:\n",
    "        self.content_col_name: str = \"content\"\n",
    "        self.content_clean_col_name: str = \"content_clean\"\n",
    "        self.content_clean_col_name_no_UNK: str = \"content_clean_no_UNK\"\n",
    "        self.content_clean_col_name_truc: str = \"content_clean_truc\"\n",
    "        self.label_col_name: str = \"label\"\n",
    "        self.train: pd.DataFrame = self.load_data(train_file_name)\n",
    "        self.test: pd.DataFrame = self.load_data(test_file_name)\n",
    "        self.evaluation: pd.DataFrame = self.load_data(evaluation_file_name)\n",
    "\n",
    "    def load_data(self, file_name: str) -> pd.DataFrame:\n",
    "        df = pd.read_csv(f\"dataset/{file_name}.csv\", sep=\";\")\n",
    "        df.dropna(inplace=True)\n",
    "        if not self.content_clean_col_name in df.columns:\n",
    "            df[self.content_col_name] = df[\"title\"] + \" \" + df[\"text\"]\n",
    "            df = df.drop(columns=[\"Unnamed: 0\"])\n",
    "        return df\n",
    "\n",
    "    def get_datasets(self) -> list[pd.DataFrame]:\n",
    "        return [self.train, self.test, self.evaluation]\n",
    "\n",
    "    def get_info(self) -> str:\n",
    "        test_info = self.test.shape\n",
    "        train_info = self.train.shape\n",
    "        evaluation_info = self.evaluation.shape\n",
    "        return f\"DataFrame Shapes:\\n\\tTrain: {train_info}\\n\\tTest: {test_info}\\n\\tEvaluation: {evaluation_info}\\n\"\n",
    "\n",
    "    def save_clean(self, token_limit: int = 10000, num_words_trunc: int = 256) -> None:\n",
    "        # CLEAN\n",
    "        self._init_clean_content([self.train, self.test, self.evaluation])\n",
    "        self.train = self.clean_df(self.train)\n",
    "        self.test = self.clean_df(self.test)\n",
    "        self.evaluation = self.clean_df(self.evaluation)\n",
    "\n",
    "        most_common_words = self.get_most_common_words_counter(\n",
    "            [self.test, self.train],\n",
    "            token_limit,\n",
    "            self.content_clean_col_name,\n",
    "        )\n",
    "        self.train = DataFrames.set_least_common_UNK(\n",
    "            self.train, \"content_clean\", most_common_words\n",
    "        )\n",
    "        self.test = DataFrames.set_least_common_UNK(\n",
    "            self.test, \"content_clean\", most_common_words\n",
    "        )\n",
    "        self.evaluation = DataFrames.set_least_common_UNK(\n",
    "            self.evaluation, \"content_clean\", most_common_words\n",
    "        )\n",
    "        self.train = DataFrames.drop_least_common(\n",
    "            self.train,\n",
    "            self.content_clean_col_name,\n",
    "            self.content_clean_col_name_no_UNK,\n",
    "            most_common_words,\n",
    "        )\n",
    "        self.test = DataFrames.drop_least_common(\n",
    "            self.test,\n",
    "            self.content_clean_col_name,\n",
    "            self.content_clean_col_name_no_UNK,\n",
    "            most_common_words,\n",
    "        )\n",
    "        self.evaluation = DataFrames.drop_least_common(\n",
    "            self.evaluation,\n",
    "            self.content_clean_col_name,\n",
    "            self.content_clean_col_name_no_UNK,\n",
    "            most_common_words,\n",
    "        )\n",
    "        self.train = DataFrames.trunc_text(\n",
    "            self.train,\n",
    "            self.content_clean_col_name_no_UNK,\n",
    "            self.content_clean_col_name_truc,\n",
    "            num_words_trunc,\n",
    "        )\n",
    "        self.test = DataFrames.trunc_text(\n",
    "            self.test,\n",
    "            self.content_clean_col_name_no_UNK,\n",
    "            self.content_clean_col_name_truc,\n",
    "            num_words_trunc,\n",
    "        )\n",
    "        self.evaluation = DataFrames.trunc_text(\n",
    "            self.evaluation,\n",
    "            self.content_clean_col_name_no_UNK,\n",
    "            self.content_clean_col_name_truc,\n",
    "            num_words_trunc,\n",
    "        )\n",
    "        # SAVE\n",
    "        cols_to_save_clean: list[str] = [\n",
    "            self.content_clean_col_name,\n",
    "            self.content_clean_col_name_no_UNK,\n",
    "            self.content_clean_col_name_truc,\n",
    "            self.label_col_name,\n",
    "        ]\n",
    "        self.train.to_csv(\n",
    "            f\"dataset/train_clean.csv\",\n",
    "            sep=\";\",\n",
    "            columns=cols_to_save_clean,\n",
    "        )\n",
    "        self.test.to_csv(\n",
    "            f\"dataset/test_clean.csv\",\n",
    "            sep=\";\",\n",
    "            columns=cols_to_save_clean,\n",
    "        )\n",
    "        self.evaluation.to_csv(\n",
    "            f\"dataset/evaluation_clean.csv\",\n",
    "            sep=\";\",\n",
    "            columns=cols_to_save_clean,\n",
    "        )\n",
    "\n",
    "    def num_unique_words(self, col_name: str) -> int:\n",
    "        result: set = set()\n",
    "        df = pd.concat([self.train, self.test, self.evaluation], ignore_index=True)\n",
    "        df[col_name].str.lower().str.split().apply(result.update)\n",
    "        return len(result)\n",
    "\n",
    "    def get_vocab(self, col_name) -> dict[str, int]:\n",
    "        vectorizer = CountVectorizer()\n",
    "        for df in [self.train, self.test, self.evaluation]:\n",
    "            vectorizer.fit_transform(df[col_name].values)\n",
    "        return vectorizer.vocabulary_\n",
    "\n",
    "    def _init_clean_content(self, dfs: list[pd.DataFrame]) -> list[pd.DataFrame]:\n",
    "        for df in dfs:\n",
    "            df[self.content_clean_col_name] = df[self.content_col_name]\n",
    "        return dfs\n",
    "\n",
    "    def clean_df(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        df = self.to_lower(df, self.content_clean_col_name)\n",
    "        df = self.remove_punctuation(df, self.content_clean_col_name)\n",
    "        df = self.remove_stopword(df, self.content_clean_col_name)\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def to_lower(df: pd.DataFrame, col_name: str) -> pd.DataFrame:\n",
    "        df[col_name] = df[col_name].apply(lambda x: str(x).lower())\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_punctuation(df: pd.DataFrame, col_name: str) -> pd.DataFrame:\n",
    "        re_punctuation = f'[{re.escape(string.punctuation)}\"”“]'\n",
    "        df[col_name] = df[col_name].apply(\n",
    "            lambda x: re.sub(re_punctuation, \" \", str(x))\n",
    "            .lower()\n",
    "            .replace(\"'s\", \"\")\n",
    "            .replace(\"’s\", \"\")\n",
    "        )\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_stopword(df: pd.DataFrame, col_name: str) -> pd.DataFrame:\n",
    "        stop_words = set(stopwords.words(\"english\"))\n",
    "        df[col_name] = df[col_name].apply(\n",
    "            lambda x: \" \".join(\n",
    "                word for word in str(x).split() if not word in stop_words\n",
    "            )\n",
    "        )\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def get_most_common_words_counter(\n",
    "        dfs: list[pd.DataFrame],\n",
    "        token_limit: int,\n",
    "        col_name: str,\n",
    "    ) -> Counter:\n",
    "        word_counter: Counter = Counter()\n",
    "        for df in dfs:\n",
    "            if col_name not in df.columns:\n",
    "                raise ValueError(\"Each DataFrame must have a 'clean_content' column\")\n",
    "            tokens = \" \".join(df[col_name].astype(str)).split()\n",
    "            word_counter.update(tokens)\n",
    "        return Counter(dict(word_counter.most_common(token_limit)))\n",
    "\n",
    "    @staticmethod\n",
    "    def set_least_common_UNK(\n",
    "        df: pd.DataFrame,\n",
    "        col_name: str,\n",
    "        most_common_words: Counter,\n",
    "    ) -> pd.DataFrame:\n",
    "        df[col_name] = df[col_name].apply(\n",
    "            lambda x: \" \".join(\n",
    "                [\n",
    "                    word if word in most_common_words else \"<UNK>\"\n",
    "                    for word in str(x).split()\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def drop_least_common(\n",
    "        df: pd.DataFrame,\n",
    "        col_name: str,\n",
    "        col_name_no_unk: str,\n",
    "        most_common_words: Counter,\n",
    "    ) -> pd.DataFrame:\n",
    "        df[col_name_no_unk] = df[col_name].apply(\n",
    "            lambda x: \" \".join(\n",
    "                [word for word in str(x).split() if word in most_common_words]\n",
    "            )\n",
    "        )\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def trunc_text(\n",
    "        df: pd.DataFrame,\n",
    "        col_name: str,\n",
    "        col_name_trunc: str,\n",
    "        trunc_num: int,\n",
    "    ) -> pd.DataFrame:\n",
    "        df[col_name_trunc] = df[col_name].apply(\n",
    "            lambda x: \" \".join(str(x).split()[:trunc_num])\n",
    "        )\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def label_to_str(label: int) -> str:\n",
    "        return \"Fake\" if label == 1 else \"Not Fake\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "807f4a87-5b06-4129-b70a-b72a91d37b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frames = DataFrames(\"train\", \"test\", \"evaluation\")\n",
    "data_frames.save_clean(token_limit=5000)\n",
    "\n",
    "data_frames_clean = DataFrames(\"train_clean\", \"test_clean\", \"evaluation_clean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d462f29-af06-4d4a-826c-90ae946d997f",
   "metadata": {},
   "source": [
    "# C-Support Vector Classification Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "93edf745-1758-4cff-8819-45e32f7fd41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6da91bcd-639c-4654-af41-12e76452eda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVCClassifier:\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_features_tfidf: int,\n",
    "        ngram_range: tuple[int, int],\n",
    "        svc_kernel: str,\n",
    "        svc_c: float,\n",
    "    ) -> None:\n",
    "        self.num_features_tfidf = num_features_tfidf\n",
    "        self.ngram_range = ngram_range\n",
    "        self.svc_kernel = svc_kernel\n",
    "        self.svc_c = svc_c\n",
    "        self.tfidf = TfidfVectorizer(\n",
    "            stop_words=\"english\",\n",
    "            max_features=num_features_tfidf,\n",
    "            ngram_range=ngram_range,\n",
    "        )\n",
    "        self.svc = SVC(\n",
    "            kernel=svc_kernel,\n",
    "            C=svc_c,\n",
    "            probability=True,\n",
    "        )\n",
    "        self.pipeline = Pipeline(\n",
    "            [\n",
    "                (\n",
    "                    \"tfidf\",\n",
    "                    self.tfidf,\n",
    "                ),\n",
    "                (\n",
    "                    \"svc\",\n",
    "                    self.svc,\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        tfidf_info: str = (\n",
    "            f\"TFIDF: (features: {self.num_features_tfidf}, ngram_range: {self.ngram_range})\"\n",
    "        )\n",
    "        svc_info: str = f\"SVC: (kernel: {self.svc_kernel}, C: {self.svc_c})\"\n",
    "        return f\"{tfidf_info}\\n{svc_info}\\n\"\n",
    "\n",
    "    def print_info(self) -> None:\n",
    "        print(self)\n",
    "\n",
    "    def train(self, X_train, y_train) -> None:\n",
    "        self.pipeline.fit(X_train, y_train)\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        return self.pipeline.predict(X_test)\n",
    "\n",
    "    def print_report(self, y_test, y_hat) -> None:\n",
    "        report = classification_report(y_test, y_hat, target_names=[\"Not Fake\", \"Fake\"])\n",
    "        print(f\"{self.__class__.__name__} Report:\\n{report}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcdccb2b-9c2b-4b24-bf07-36e96a780799",
   "metadata": {},
   "source": [
    "# Test Runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f346f720-0bc7-4aec-ba35-3b4a6cf017f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ce1a4732-18b0-49a5-9b93-4e33ba69a54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_svc(\n",
    "    data_frames: DataFrames,\n",
    "    content_col_name: str,\n",
    "    label_col_name: str,\n",
    "    num_features_tfidf=1000,\n",
    "    ngram_range=(1, 2),\n",
    ") -> float:\n",
    "    svc_classifier = SVCClassifier(\n",
    "        num_features_tfidf=num_features_tfidf,\n",
    "        ngram_range=ngram_range,\n",
    "        svc_kernel=\"linear\",\n",
    "        svc_c=1.0,\n",
    "    )\n",
    "    svc_classifier.print_info()\n",
    "    start_time = time.time()\n",
    "    svc_classifier.train(\n",
    "        data_frames.train[content_col_name],\n",
    "        data_frames.train[label_col_name],\n",
    "    )\n",
    "    end_time = time.time()\n",
    "    y_hat = svc_classifier.predict(data_frames.test[content_col_name])\n",
    "    svc_classifier.print_report(data_frames.test[label_col_name], y_hat)\n",
    "    return end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c820f463-a99a-43f2-9005-d978ddc2ba0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTS WITH TIME\n",
    "times: dict[str, float] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3b7881ed-cf5a-4229-b023-e20cc119f9ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFIDF: (features: 1000, ngram_range: (1, 2))\n",
      "SVC: (kernel: linear, C: 1.0)\n",
      "\n",
      "SVCClassifier Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Not Fake       0.97      0.98      0.98      3753\n",
      "        Fake       0.99      0.97      0.98      4364\n",
      "\n",
      "    accuracy                           0.98      8117\n",
      "   macro avg       0.98      0.98      0.98      8117\n",
      "weighted avg       0.98      0.98      0.98      8117\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svc_time: float = run_svc(\n",
    "    data_frames,\n",
    "    data_frames.content_col_name,\n",
    "    data_frames_clean.label_col_name\n",
    ")\n",
    "times[\"SVC\"] = svc_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00f83c7a-a5d4-4ff2-b7b1-4c63e8c09893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFIDF: (features: 1000, ngram_range: (1, 2))\n",
      "SVC: (kernel: linear, C: 1.0)\n",
      "\n",
      "SVCClassifier Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Not Fake       0.97      0.99      0.98      3753\n",
      "        Fake       0.99      0.97      0.98      4364\n",
      "\n",
      "    accuracy                           0.98      8117\n",
      "   macro avg       0.98      0.98      0.98      8117\n",
      "weighted avg       0.98      0.98      0.98      8117\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svc_time_clean: float = run_svc(\n",
    "    data_frames_clean,\n",
    "    data_frames_clean.content_clean_col_name_truc,\n",
    "    data_frames_clean.label_col_name,\n",
    ")\n",
    "times[\"SVC Clean\"] = svc_time_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "868025f1-b676-4cc5-a8ad-6b44684c1a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC: 278.58794 seconds\n",
      "SVC Clean: 256.29854 seconds\n"
     ]
    }
   ],
   "source": [
    "for fname, ftime in times.items():\n",
    "    print(f\"{fname}: {ftime:.5f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baddf3ef-9e93-4dc6-ba07-bb8d58948325",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Fake News Env)",
   "language": "python",
   "name": "fake-news-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
