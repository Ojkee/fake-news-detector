{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f02e278c-5b1a-4d4f-86ac-7adf15d5e0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d188aa9-4adc-41f8-9585-157077175b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataFrames:\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_file_name: str,\n",
    "        test_file_name: str,\n",
    "        evaluation_file_name: str,\n",
    "    ) -> None:\n",
    "        self.content_col_name: str = \"content\"\n",
    "        self.content_clean_col_name: str = \"content_clean\"\n",
    "        self.content_clean_col_name_no_UNK: str = \"content_clean_no_UNK\"\n",
    "        self.content_clean_col_name_truc: str = \"content_clean_truc\"\n",
    "        self.label_col_name: str = \"label\"\n",
    "        self.train: pd.DataFrame = self.load_data(train_file_name)\n",
    "        self.test: pd.DataFrame = self.load_data(test_file_name)\n",
    "        self.evaluation: pd.DataFrame = self.load_data(evaluation_file_name)\n",
    "\n",
    "    def load_data(self, file_name: str) -> pd.DataFrame:\n",
    "        df = pd.read_csv(f\"dataset/{file_name}.csv\", sep=\";\")\n",
    "        df.dropna(inplace=True)\n",
    "        if not self.content_clean_col_name in df.columns:\n",
    "            df[self.content_col_name] = df[\"title\"] + \" \" + df[\"text\"]\n",
    "            df = df.drop(columns=[\"Unnamed: 0\"])\n",
    "        return df\n",
    "\n",
    "    def get_datasets(self) -> list[pd.DataFrame]:\n",
    "        return [self.train, self.test, self.evaluation]\n",
    "\n",
    "    def get_info(self) -> str:\n",
    "        test_info = self.test.shape\n",
    "        train_info = self.train.shape\n",
    "        evaluation_info = self.evaluation.shape\n",
    "        return f\"DataFrame Shapes:\\n\\tTrain: {train_info}\\n\\tTest: {test_info}\\n\\tEvaluation: {evaluation_info}\\n\"\n",
    "\n",
    "    def save_clean(self, token_limit: int = 10000, num_words_trunc: int = 256) -> None:\n",
    "        # CLEAN\n",
    "        self._init_clean_content([self.train, self.test, self.evaluation])\n",
    "        self.train = self.clean_df(self.train)\n",
    "        self.test = self.clean_df(self.test)\n",
    "        self.evaluation = self.clean_df(self.evaluation)\n",
    "\n",
    "        most_common_words = self.get_most_common_words_counter(\n",
    "            [self.test, self.train],\n",
    "            token_limit,\n",
    "            self.content_clean_col_name,\n",
    "        )\n",
    "        self.train = DataFrames.set_least_common_UNK(\n",
    "            self.train, \"content_clean\", most_common_words\n",
    "        )\n",
    "        self.test = DataFrames.set_least_common_UNK(\n",
    "            self.test, \"content_clean\", most_common_words\n",
    "        )\n",
    "        self.evaluation = DataFrames.set_least_common_UNK(\n",
    "            self.evaluation, \"content_clean\", most_common_words\n",
    "        )\n",
    "        self.train = DataFrames.drop_least_common(\n",
    "            self.train,\n",
    "            self.content_clean_col_name,\n",
    "            self.content_clean_col_name_no_UNK,\n",
    "            most_common_words,\n",
    "        )\n",
    "        self.test = DataFrames.drop_least_common(\n",
    "            self.test,\n",
    "            self.content_clean_col_name,\n",
    "            self.content_clean_col_name_no_UNK,\n",
    "            most_common_words,\n",
    "        )\n",
    "        self.evaluation = DataFrames.drop_least_common(\n",
    "            self.evaluation,\n",
    "            self.content_clean_col_name,\n",
    "            self.content_clean_col_name_no_UNK,\n",
    "            most_common_words,\n",
    "        )\n",
    "        self.train = DataFrames.trunc_text(\n",
    "            self.train,\n",
    "            self.content_clean_col_name_no_UNK,\n",
    "            self.content_clean_col_name_truc,\n",
    "            num_words_trunc,\n",
    "        )\n",
    "        self.test = DataFrames.trunc_text(\n",
    "            self.test,\n",
    "            self.content_clean_col_name_no_UNK,\n",
    "            self.content_clean_col_name_truc,\n",
    "            num_words_trunc,\n",
    "        )\n",
    "        self.evaluation = DataFrames.trunc_text(\n",
    "            self.evaluation,\n",
    "            self.content_clean_col_name_no_UNK,\n",
    "            self.content_clean_col_name_truc,\n",
    "            num_words_trunc,\n",
    "        )\n",
    "        print(self.test.columns)\n",
    "        # SAVE\n",
    "        cols_to_save_clean: list[str] = [\n",
    "            self.content_clean_col_name,\n",
    "            self.content_clean_col_name_no_UNK,\n",
    "            self.content_clean_col_name_truc,\n",
    "            self.label_col_name,\n",
    "        ]\n",
    "        self.train.to_csv(\n",
    "            f\"dataset/train_clean.csv\",\n",
    "            sep=\";\",\n",
    "            columns=cols_to_save_clean,\n",
    "        )\n",
    "        self.test.to_csv(\n",
    "            f\"dataset/test_clean.csv\",\n",
    "            sep=\";\",\n",
    "            columns=cols_to_save_clean,\n",
    "        )\n",
    "        self.evaluation.to_csv(\n",
    "            f\"dataset/evaluation_clean.csv\",\n",
    "            sep=\";\",\n",
    "            columns=cols_to_save_clean,\n",
    "        )\n",
    "\n",
    "    def num_unique_words(self, col_name: str) -> int:\n",
    "        result: set = set()\n",
    "        df = pd.concat([self.train, self.test, self.evaluation], ignore_index=True)\n",
    "        df[col_name].str.lower().str.split().apply(result.update)\n",
    "        return len(result)\n",
    "\n",
    "    def get_vocab(self, col_name) -> dict[str, int]:\n",
    "        vectorizer = CountVectorizer()\n",
    "        for df in [self.train, self.test, self.evaluation]:\n",
    "            vectorizer.fit_transform(df[col_name].values)\n",
    "        return vectorizer.vocabulary_\n",
    "\n",
    "    def _init_clean_content(self, dfs: list[pd.DataFrame]) -> list[pd.DataFrame]:\n",
    "        for df in dfs:\n",
    "            df[self.content_clean_col_name] = df[self.content_col_name]\n",
    "        return dfs\n",
    "\n",
    "    def clean_df(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        df = self.to_lower(df, self.content_clean_col_name)\n",
    "        df = self.remove_punctuation(df, self.content_clean_col_name)\n",
    "        df = self.remove_stopword(df, self.content_clean_col_name)\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def to_lower(df: pd.DataFrame, col_name: str) -> pd.DataFrame:\n",
    "        df[col_name] = df[col_name].apply(lambda x: str(x).lower())\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_punctuation(df: pd.DataFrame, col_name: str) -> pd.DataFrame:\n",
    "        re_punctuation = f'[{re.escape(string.punctuation)}\"”“]'\n",
    "        df[col_name] = df[col_name].apply(\n",
    "            lambda x: re.sub(re_punctuation, \" \", str(x))\n",
    "            .lower()\n",
    "            .replace(\"'s\", \"\")\n",
    "            .replace(\"’s\", \"\")\n",
    "        )\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_stopword(df: pd.DataFrame, col_name: str) -> pd.DataFrame:\n",
    "        stop_words = set(stopwords.words(\"english\"))\n",
    "        df[col_name] = df[col_name].apply(\n",
    "            lambda x: \" \".join(\n",
    "                word for word in str(x).split() if not word in stop_words\n",
    "            )\n",
    "        )\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def get_most_common_words_counter(\n",
    "        dfs: list[pd.DataFrame],\n",
    "        token_limit: int,\n",
    "        col_name: str,\n",
    "    ) -> Counter:\n",
    "        word_counter: Counter = Counter()\n",
    "        for df in dfs:\n",
    "            if col_name not in df.columns:\n",
    "                raise ValueError(\"Each DataFrame must have a 'clean_content' column\")\n",
    "            tokens = \" \".join(df[col_name].astype(str)).split()\n",
    "            word_counter.update(tokens)\n",
    "        return Counter(dict(word_counter.most_common(token_limit)))\n",
    "\n",
    "    @staticmethod\n",
    "    def set_least_common_UNK(\n",
    "        df: pd.DataFrame,\n",
    "        col_name: str,\n",
    "        most_common_words: Counter,\n",
    "    ) -> pd.DataFrame:\n",
    "        df[col_name] = df[col_name].apply(\n",
    "            lambda x: \" \".join(\n",
    "                [\n",
    "                    word if word in most_common_words else \"<UNK>\"\n",
    "                    for word in str(x).split()\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def drop_least_common(\n",
    "        df: pd.DataFrame,\n",
    "        col_name: str,\n",
    "        col_name_no_unk: str,\n",
    "        most_common_words: Counter,\n",
    "    ) -> pd.DataFrame:\n",
    "        df[col_name_no_unk] = df[col_name].apply(\n",
    "            lambda x: \" \".join(\n",
    "                [word for word in str(x).split() if word in most_common_words]\n",
    "            )\n",
    "        )\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def trunc_text(\n",
    "        df: pd.DataFrame,\n",
    "        col_name: str,\n",
    "        col_name_trunc: str,\n",
    "        trunc_num: int,\n",
    "    ) -> pd.DataFrame:\n",
    "        df[col_name_trunc] = df[col_name].apply(\n",
    "            lambda x: \" \".join(str(x).split()[:trunc_num])\n",
    "        )\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def label_to_str(label: int) -> str:\n",
    "        return \"Fake\" if label == 1 else \"Not Fake\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "82984d57-bff7-4fd8-ae13-9f5c6fea3ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c9eda384-57a6-4794-ab71-4fb8f402e608",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionClassifier:\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_features_tfidf: int,\n",
    "        ngram_range: tuple[int, int],\n",
    "        logistic_regression_max_iter: int,\n",
    "    ) -> None:\n",
    "        self.num_features_tfidf = num_features_tfidf\n",
    "        self.ngram_range = ngram_range\n",
    "        self.logistic_regression_max_iter = logistic_regression_max_iter\n",
    "        self.tfidf = TfidfVectorizer(\n",
    "            stop_words=\"english\",\n",
    "            max_features=num_features_tfidf,\n",
    "            ngram_range=ngram_range,\n",
    "        )\n",
    "        self.logistic_regression = LogisticRegression(\n",
    "            max_iter=logistic_regression_max_iter,\n",
    "        )\n",
    "        self.pipeline = Pipeline(\n",
    "            [\n",
    "                (\n",
    "                    \"tfidf\",\n",
    "                    self.tfidf,\n",
    "                ),\n",
    "                (\n",
    "                    \"logistic-regression\",\n",
    "                    self.logistic_regression,\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "        self.y_prediction = ...\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        tfidf_info: str = (\n",
    "            f\"TFIDF: (features: {self.num_features_tfidf}, ngram_range: {self.ngram_range})\"\n",
    "        )\n",
    "        log_reg_info: str = (\n",
    "            f\"LOGISTIC-REGRESSION: (max iter: {self.logistic_regression_max_iter})\"\n",
    "        )\n",
    "        return f\"{tfidf_info}\\n{log_reg_info}\\n\"\n",
    "\n",
    "    def print_info(self) -> None:\n",
    "        print(self)\n",
    "\n",
    "    def train(self, X_train, y_train) -> None:\n",
    "        self.pipeline.fit(X_train, y_train)\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        return self.pipeline.predict(X_test)\n",
    "\n",
    "    def print_report(self, y_test, y_hat) -> None:\n",
    "        report = classification_report(y_test, y_hat, target_names=[\"Not Fake\", \"Fake\"])\n",
    "        print(f\"{self.__class__.__name__} Report:\\n{report}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dcae542a-8481-468a-8e9d-586f4631fe89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6adab66f-ca18-4fd7-a54d-b2b7ccafe58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_lgc(\n",
    "    data_frames: DataFrames,\n",
    "    content_col_name: str,\n",
    "    label_col_name: str,\n",
    "    num_features_tfidf=2000,\n",
    "    ngram_range=(1, 2),\n",
    "    logistic_regression_max_iter=1000,\n",
    ") -> float:\n",
    "    lgc = LogisticRegressionClassifier(\n",
    "        num_features_tfidf,\n",
    "        ngram_range,\n",
    "        logistic_regression_max_iter,\n",
    "    )\n",
    "    start_time_lgc = time.time()\n",
    "    lgc.train(\n",
    "        data_frames.train[content_col_name],\n",
    "        data_frames.train[label_col_name],\n",
    "    )\n",
    "    end_time_lgc = time.time()\n",
    "    prediction = lgc.predict(data_frames.test[content_col_name])\n",
    "    lgc.print_info()\n",
    "    lgc.print_report(prediction, data_frames.test[label_col_name])\n",
    "\n",
    "    rand_sample = data_frames.evaluation.sample(1)\n",
    "    pred_sample = lgc.predict(rand_sample[content_col_name])\n",
    "    print(rand_sample[content_col_name].values)\n",
    "    label_str_true: str = data_frames.label_to_str(\n",
    "        rand_sample[label_col_name].values[0]\n",
    "    )\n",
    "    label_str_predicted: str = data_frames.label_to_str(pred_sample[0])\n",
    "    print(f\"is:        {label_str_true}\\npredicted: {label_str_predicted}\")\n",
    "    return end_time_lgc - start_time_lgc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "330a54fc-93ef-463f-8e32-b82905934d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words dataframes: 394396\n",
      "Index(['title', 'text', 'label', 'content', 'content_clean',\n",
      "       'content_clean_no_UNK', 'content_clean_truc'],\n",
      "      dtype='object')\n",
      "Unique words dataframes clean: 1001\n"
     ]
    }
   ],
   "source": [
    "# DATA\n",
    "data_frames = DataFrames(\"train\", \"test\", \"evaluation\")\n",
    "data_frames_unique = data_frames.num_unique_words(\"content\")\n",
    "print(f\"Unique words dataframes: {data_frames_unique}\")\n",
    "data_frames.save_clean(token_limit=1000)\n",
    "\n",
    "data_frames_clean = DataFrames(\"train_clean\", \"test_clean\", \"evaluation_clean\")\n",
    "data_frames_clean_unique = data_frames_clean.num_unique_words(\"content_clean\")\n",
    "print(f\"Unique words dataframes clean: {data_frames_clean_unique}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e7545845-776f-42eb-b9f8-432a30d94174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTS WITH TIME\n",
    "times: dict[str, float] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3ba59a0f-9c13-4855-8a08-821d8184809c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFIDF: (features: 2000, ngram_range: (1, 2))\n",
      "LOGISTIC-REGRESSION: (max iter: 1000)\n",
      "\n",
      "LogisticRegressionClassifier Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Not Fake       0.97      0.97      0.97      3751\n",
      "        Fake       0.97      0.97      0.97      4366\n",
      "\n",
      "    accuracy                           0.97      8117\n",
      "   macro avg       0.97      0.97      0.97      8117\n",
      "weighted avg       0.97      0.97      0.97      8117\n",
      "\n",
      "['Hypothetically speaking, U.S. Admiral says ready for nuclear strike on China if Trump so ordered MELBOURNE (Reuters) - The U.S. Pacific Fleet commander, addressing a security conference in Australia, said in answer to a question on Thursday that he would be prepared to launch a nuclear strike on China if President Donald Trump so ordered. The fleet spokesman later said the question was asked as an “outrageous hypothetical”. Admiral Scott Swift was speaking at the Australian National University in Canberra when he was asked whether he would be prepared to launch a nuclear attack on China if ordered to do so by Trump. “The answer would be yes,” he said.  Swift said that all members of the U.S. military had sworn an oath to obey officers and the U.S. president as commander in chief to defend the constitution. “This is core to the American democracy,” he said, in a recording of the event obtained by Reuters. “Any time you have a military that is moving away from a focus, and an allegiance, to civilian control, then we really have significant problems.”   Swift’s answer reaffirmed the principle of civilian control over the military and was based on an “outrageous hypothetical” in the question, Pacific Fleet spokesman Captain Charlie Brown told Reuters. “Frankly, the premise of the question was ridiculous,” he said. “It was posed as an outrageous hypothetical, but the admiral simply took it as an opportunity to say the fact is that we have civilian control of the military and we abide by that principle.” Speaking in Beijing on Friday, a spokesman of China’s Foreign Ministry also downplayed the remark. “Many people have paid attention to this but the spokesman for the Pacific Fleet has pointed out the ridiculousness of this report,” Lu Kang told a daily news briefing.  The United States and China enjoy a generally friendly relationship, with strong economic ties, albeit with frequent barbs about trade, jobs, currencies, human rights, Tibet, the South China Sea and North Korea. Trump has held high hopes for greater cooperation from China to exert influence over North Korea, leaning heavily on Chinese President Xi Jinping for his assistance. The two leaders had a high-profile summit in Florida in April and Trump has frequently praised Xi. ']\n",
      "is:        Fake\n",
      "predicted: Fake\n"
     ]
    }
   ],
   "source": [
    "lgc_time: float = run_lgc(\n",
    "    data_frames,\n",
    "    data_frames.content_col_name,\n",
    "    data_frames_clean.label_col_name\n",
    ")\n",
    "times[\"LGC\"] = lgc_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6a0ab04b-d090-4035-b808-5c82e4606b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFIDF: (features: 2000, ngram_range: (1, 2))\n",
      "LOGISTIC-REGRESSION: (max iter: 1000)\n",
      "\n",
      "LogisticRegressionClassifier Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Not Fake       0.97      0.97      0.97      3754\n",
      "        Fake       0.97      0.97      0.97      4359\n",
      "\n",
      "    accuracy                           0.97      8113\n",
      "   macro avg       0.97      0.97      0.97      8113\n",
      "weighted avg       0.97      0.97      0.97      8113\n",
      "\n",
      "['watch republicans hard cnn pass house republicans seven years come plan care act plan bad even gop majority could pass voting care act times years order conservative base republicans actually american healthcare act would health care 20 million americans many republican voters reason bill would also cause healthcare especially senior citizens would pay times much right care act vote republican bill thursday republican leadership could enough members party get number votes vote republicans control house even bring bill vote simply put donald trump found face immediately house republicans vote friday trump major policy republicans care act great idea adding thursday cnn long republicans america first black president lot republican leaders say said seven years ago today care act obamacare signed law president obama since time republicans day could obamacare nation health care seven years day weeks talk house leaders white house secretary bill pass house plan well bill republican leaders vote calling move thing would actually bring vote republican leaders apparently would video via bill pass trump republicans victory american people millions care act help get keep health insurance friday americans need republican representatives vote bill trump able voting voting anything america featured image via']\n",
      "is:        Not Fake\n",
      "predicted: Not Fake\n"
     ]
    }
   ],
   "source": [
    "lgc_time_clean: float = run_lgc(\n",
    "    data_frames_clean,\n",
    "    data_frames_clean.content_clean_col_name_truc,\n",
    "    data_frames_clean.label_col_name,\n",
    ")\n",
    "times[\"LGC Clean\"] = lgc_time_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3d0bc835-c060-4b16-a9a9-a3693341eeaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGC: 12.72050 seconds\n",
      "LGC Clean: 3.03294 seconds\n"
     ]
    }
   ],
   "source": [
    "for fname, ftime in times.items():\n",
    "    print(f\"{fname}: {ftime:.5f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ae659d-3702-4827-8a50-a2e4ec4877c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Fake News Env)",
   "language": "python",
   "name": "fake-news-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
