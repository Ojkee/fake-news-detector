{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fed4349c-a241-439a-b1b1-676558ec2253",
   "metadata": {},
   "source": [
    "# Essential Global imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eed90cb4-cf58-42f7-8aa8-d3707ed901e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8907c0c-03f1-495f-a047-c6138820bda5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1df7a60-6bfd-4fa5-873d-cabe951f2390",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0538d955-d671-4336-bbfd-fc2b89aae981",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataFrames:\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_file_name: str,\n",
    "        test_file_name: str,\n",
    "        evaluation_file_name: str,\n",
    "    ) -> None:\n",
    "        self.content_col_name: str = \"content\"\n",
    "        self.content_clean_col_name: str = \"content_clean\"\n",
    "        self.content_clean_col_name_no_UNK: str = \"content_clean_no_UNK\"\n",
    "        self.content_clean_col_name_truc: str = \"content_clean_truc\"\n",
    "        self.label_col_name: str = \"label\"\n",
    "        self.train: pd.DataFrame = self.load_data(train_file_name)\n",
    "        self.test: pd.DataFrame = self.load_data(test_file_name)\n",
    "        self.evaluation: pd.DataFrame = self.load_data(evaluation_file_name)\n",
    "\n",
    "    def load_data(self, file_name: str) -> pd.DataFrame:\n",
    "        df = pd.read_csv(f\"dataset/{file_name}.csv\", sep=\";\")\n",
    "        df.dropna(inplace=True)\n",
    "        if not self.content_clean_col_name in df.columns:\n",
    "            df[self.content_col_name] = df[\"title\"] + \" \" + df[\"text\"]\n",
    "            df = df.drop(columns=[\"Unnamed: 0\"])\n",
    "        return df\n",
    "\n",
    "    def get_datasets(self) -> list[pd.DataFrame]:\n",
    "        return [self.train, self.test, self.evaluation]\n",
    "\n",
    "    def get_info(self) -> str:\n",
    "        test_info = self.test.shape\n",
    "        train_info = self.train.shape\n",
    "        evaluation_info = self.evaluation.shape\n",
    "        return f\"DataFrame Shapes:\\n\\tTrain: {train_info}\\n\\tTest: {test_info}\\n\\tEvaluation: {evaluation_info}\\n\"\n",
    "\n",
    "    def save_clean(self, file_name: str, token_limit: int = 10000, num_words_trunc: int = 256) -> None:\n",
    "        # CLEAN\n",
    "        self._init_clean_content([self.train, self.test, self.evaluation])\n",
    "        self.train = self.clean_df(self.train)\n",
    "        self.test = self.clean_df(self.test)\n",
    "        self.evaluation = self.clean_df(self.evaluation)\n",
    "\n",
    "        most_common_words = self.get_most_common_words_counter(\n",
    "            [self.test, self.train],\n",
    "            token_limit,\n",
    "            self.content_clean_col_name,\n",
    "        )\n",
    "        self.train = DataFrames.set_least_common_UNK(\n",
    "            self.train, \"content_clean\", most_common_words\n",
    "        )\n",
    "        self.test = DataFrames.set_least_common_UNK(\n",
    "            self.test, \"content_clean\", most_common_words\n",
    "        )\n",
    "        self.evaluation = DataFrames.set_least_common_UNK(\n",
    "            self.evaluation, \"content_clean\", most_common_words\n",
    "        )\n",
    "        self.train = DataFrames.drop_least_common(\n",
    "            self.train,\n",
    "            self.content_clean_col_name,\n",
    "            self.content_clean_col_name_no_UNK,\n",
    "            most_common_words,\n",
    "        )\n",
    "        self.test = DataFrames.drop_least_common(\n",
    "            self.test,\n",
    "            self.content_clean_col_name,\n",
    "            self.content_clean_col_name_no_UNK,\n",
    "            most_common_words,\n",
    "        )\n",
    "        self.evaluation = DataFrames.drop_least_common(\n",
    "            self.evaluation,\n",
    "            self.content_clean_col_name,\n",
    "            self.content_clean_col_name_no_UNK,\n",
    "            most_common_words,\n",
    "        )\n",
    "        self.train = DataFrames.trunc_text(\n",
    "            self.train,\n",
    "            self.content_clean_col_name_no_UNK,\n",
    "            self.content_clean_col_name_truc,\n",
    "            num_words_trunc,\n",
    "        )\n",
    "        self.test = DataFrames.trunc_text(\n",
    "            self.test,\n",
    "            self.content_clean_col_name_no_UNK,\n",
    "            self.content_clean_col_name_truc,\n",
    "            num_words_trunc,\n",
    "        )\n",
    "        self.evaluation = DataFrames.trunc_text(\n",
    "            self.evaluation,\n",
    "            self.content_clean_col_name_no_UNK,\n",
    "            self.content_clean_col_name_truc,\n",
    "            num_words_trunc,\n",
    "        )\n",
    "        # SAVE\n",
    "        cols_to_save_clean: list[str] = [\n",
    "            self.content_clean_col_name,\n",
    "            self.content_clean_col_name_no_UNK,\n",
    "            self.content_clean_col_name_truc,\n",
    "            self.label_col_name,\n",
    "        ]\n",
    "        self.train.to_csv(\n",
    "            f\"dataset/train_clean_{file_name}.csv\",\n",
    "            sep=\";\",\n",
    "            columns=cols_to_save_clean,\n",
    "        )\n",
    "        self.test.to_csv(\n",
    "            f\"dataset/test_clean.csv_{file_name}\",\n",
    "            sep=\";\",\n",
    "            columns=cols_to_save_clean,\n",
    "        )\n",
    "        self.evaluation.to_csv(\n",
    "            f\"dataset/evaluation_clean.csv_{file_name}\",\n",
    "            sep=\";\",\n",
    "            columns=cols_to_save_clean,\n",
    "        )\n",
    "\n",
    "    def num_unique_words(self, col_name: str) -> int:\n",
    "        result: set = set()\n",
    "        df = pd.concat([self.train, self.test, self.evaluation], ignore_index=True)\n",
    "        df[col_name].str.lower().str.split().apply(result.update)\n",
    "        return len(result)\n",
    "\n",
    "    def get_vocab(self, col_name) -> dict[str, int]:\n",
    "        vectorizer = CountVectorizer()\n",
    "        for df in [self.train, self.test, self.evaluation]:\n",
    "            vectorizer.fit_transform(df[col_name].values)\n",
    "        return vectorizer.vocabulary_\n",
    "\n",
    "    def _init_clean_content(self, dfs: list[pd.DataFrame]) -> list[pd.DataFrame]:\n",
    "        for df in dfs:\n",
    "            df[self.content_clean_col_name] = df[self.content_col_name]\n",
    "        return dfs\n",
    "\n",
    "    def clean_df(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        df = self.to_lower(df, self.content_clean_col_name)\n",
    "        df = self.remove_punctuation(df, self.content_clean_col_name)\n",
    "        df = self.remove_stopword(df, self.content_clean_col_name)\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def to_lower(df: pd.DataFrame, col_name: str) -> pd.DataFrame:\n",
    "        df[col_name] = df[col_name].apply(lambda x: str(x).lower())\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_punctuation(df: pd.DataFrame, col_name: str) -> pd.DataFrame:\n",
    "        re_punctuation = f'[{re.escape(string.punctuation)}\"”“]'\n",
    "        df[col_name] = df[col_name].apply(\n",
    "            lambda x: re.sub(re_punctuation, \" \", str(x))\n",
    "            .lower()\n",
    "            .replace(\"'s\", \"\")\n",
    "            .replace(\"’s\", \"\")\n",
    "        )\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_stopword(df: pd.DataFrame, col_name: str) -> pd.DataFrame:\n",
    "        stop_words = set(stopwords.words(\"english\"))\n",
    "        df[col_name] = df[col_name].apply(\n",
    "            lambda x: \" \".join(\n",
    "                word for word in str(x).split() if not word in stop_words\n",
    "            )\n",
    "        )\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def get_most_common_words_counter(\n",
    "        dfs: list[pd.DataFrame],\n",
    "        token_limit: int,\n",
    "        col_name: str,\n",
    "    ) -> Counter:\n",
    "        word_counter: Counter = Counter()\n",
    "        for df in dfs:\n",
    "            if col_name not in df.columns:\n",
    "                raise ValueError(\"Each DataFrame must have a 'clean_content' column\")\n",
    "            tokens = \" \".join(df[col_name].astype(str)).split()\n",
    "            word_counter.update(tokens)\n",
    "        return Counter(dict(word_counter.most_common(token_limit)))\n",
    "\n",
    "    @staticmethod\n",
    "    def set_least_common_UNK(\n",
    "        df: pd.DataFrame,\n",
    "        col_name: str,\n",
    "        most_common_words: Counter,\n",
    "    ) -> pd.DataFrame:\n",
    "        df[col_name] = df[col_name].apply(\n",
    "            lambda x: \" \".join(\n",
    "                [\n",
    "                    word if word in most_common_words else \"<UNK>\"\n",
    "                    for word in str(x).split()\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def drop_least_common(\n",
    "        df: pd.DataFrame,\n",
    "        col_name: str,\n",
    "        col_name_no_unk: str,\n",
    "        most_common_words: Counter,\n",
    "    ) -> pd.DataFrame:\n",
    "        df[col_name_no_unk] = df[col_name].apply(\n",
    "            lambda x: \" \".join(\n",
    "                [word for word in str(x).split() if word in most_common_words]\n",
    "            )\n",
    "        )\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def trunc_text(\n",
    "        df: pd.DataFrame,\n",
    "        col_name: str,\n",
    "        col_name_trunc: str,\n",
    "        trunc_num: int,\n",
    "    ) -> pd.DataFrame:\n",
    "        df[col_name_trunc] = df[col_name].apply(\n",
    "            lambda x: \" \".join(str(x).split()[:trunc_num])\n",
    "        )\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def label_to_str(label: int) -> str:\n",
    "        return \"Fake\" if label == 1 else \"Not Fake\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "9d54e56d-1999-4352-860e-582f7624a030",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frames = DataFrames(\"train\", \"test\", \"evaluation\")\n",
    "data_frames.save_clean(\"1000tok\", token_limit=1000)\n",
    "\n",
    "data_frames_1000tok = DataFrames(\"train_clean\", \"test_clean\", \"evaluation_clean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a9e81c-3857-4024-9dc1-ba8627af36a6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c713aade-cfa2-43a5-b41e-f34c29fc1a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c8319bbb-beb5-4751-9743-cc0a8970bbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionClassifier:\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_features_tfidf: int,\n",
    "        ngram_range: tuple[int, int],\n",
    "        logistic_regression_max_iter: int,\n",
    "    ) -> None:\n",
    "        self.num_features_tfidf = num_features_tfidf\n",
    "        self.ngram_range = ngram_range\n",
    "        self.logistic_regression_max_iter = logistic_regression_max_iter\n",
    "        self.tfidf = TfidfVectorizer(\n",
    "            stop_words=\"english\",\n",
    "            max_features=num_features_tfidf,\n",
    "            ngram_range=ngram_range,\n",
    "        )\n",
    "        self.logistic_regression = LogisticRegression(\n",
    "            max_iter=logistic_regression_max_iter,\n",
    "        )\n",
    "        self.pipeline = Pipeline(\n",
    "            [\n",
    "                (\n",
    "                    \"tfidf\",\n",
    "                    self.tfidf,\n",
    "                ),\n",
    "                (\n",
    "                    \"logistic-regression\",\n",
    "                    self.logistic_regression,\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "        self.y_prediction = ...\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        tfidf_info: str = (\n",
    "            f\"TFIDF: {self.num_features_tfidf}, {self.ngram_range})\"\n",
    "        )\n",
    "        log_reg_info: str = (\n",
    "            f\"L-REG: {self.logistic_regression_max_iter})\"\n",
    "        )\n",
    "        return f\"{tfidf_info} {log_reg_info}\"\n",
    "\n",
    "    def print_info(self) -> None:\n",
    "        print(self)\n",
    "\n",
    "    def train(self, X_train, y_train) -> None:\n",
    "        self.pipeline.fit(X_train, y_train)\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        return self.pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "839365d9-1d45-421c-bb29-2b8b93d7da47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_lrc(\n",
    "    data_frames: DataFrames,\n",
    "    content_col_name: str,\n",
    "    label_col_name: str,\n",
    "    num_features_tfidf=2000,\n",
    "    ngram_range=(1, 2),\n",
    "    logistic_regression_max_iter=1000,\n",
    ") -> list:\n",
    "    lrc = LogisticRegressionClassifier(\n",
    "        num_features_tfidf,\n",
    "        ngram_range,\n",
    "        logistic_regression_max_iter,\n",
    "    )\n",
    "    start_time = time.time()\n",
    "    lrc.train(\n",
    "        data_frames.train[content_col_name],\n",
    "        data_frames.train[label_col_name],\n",
    "    )\n",
    "    end_time = time.time()\n",
    "    \n",
    "    y_hat = lrc.predict(data_frames.test[content_col_name])\n",
    "    \n",
    "    mtime = end_time - start_time\n",
    "    acc = accuracy_score(y_hat, data_frames.test[label_col_name])\n",
    "    desc = str(lrc)\n",
    "    return [\"LRC\", \"cpu\", mtime, acc, desc, content_col_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee989e3-d582-47fd-aff1-cedaef003e72",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# C-Support Vector Classification Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e9fcfdcc-d801-424a-9177-c1e8f462ad0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "75d0ba3c-3c80-42d7-80fa-21b1704be55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVCClassifier:\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_features_tfidf: int,\n",
    "        ngram_range: tuple[int, int],\n",
    "        svc_kernel: str,\n",
    "        svc_c: float,\n",
    "    ) -> None:\n",
    "        self.num_features_tfidf = num_features_tfidf\n",
    "        self.ngram_range = ngram_range\n",
    "        self.svc_kernel = svc_kernel\n",
    "        self.svc_c = svc_c\n",
    "        self.tfidf = TfidfVectorizer(\n",
    "            stop_words=\"english\",\n",
    "            max_features=num_features_tfidf,\n",
    "            ngram_range=ngram_range,\n",
    "        )\n",
    "        self.svc = SVC(\n",
    "            kernel=svc_kernel,\n",
    "            C=svc_c,\n",
    "            probability=True,\n",
    "        )\n",
    "        self.pipeline = Pipeline(\n",
    "            [\n",
    "                (\n",
    "                    \"tfidf\",\n",
    "                    self.tfidf,\n",
    "                ),\n",
    "                (\n",
    "                    \"svc\",\n",
    "                    self.svc,\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        tfidf_info: str = (\n",
    "            f\"TFIDF: {self.num_features_tfidf}, {self.ngram_range})\"\n",
    "        )\n",
    "        svc_info: str = f\"SVC: {self.svc_kernel}, C: {self.svc_c})\"\n",
    "        return f\"{tfidf_info} {svc_info}\"\n",
    "\n",
    "    def print_info(self) -> None:\n",
    "        print(self)\n",
    "\n",
    "    def train(self, X_train, y_train) -> None:\n",
    "        self.pipeline.fit(X_train, y_train)\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        return self.pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "8db8ef30-c0f4-4617-b3d3-0a1d142dcca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_svc(\n",
    "    data_frames: DataFrames,\n",
    "    content_col_name: str,\n",
    "    label_col_name: str,\n",
    "    num_features_tfidf=1000,\n",
    "    ngram_range=(1, 2),\n",
    ") -> list:\n",
    "    svc = SVCClassifier(\n",
    "        num_features_tfidf=num_features_tfidf,\n",
    "        ngram_range=ngram_range,\n",
    "        svc_kernel=\"linear\",\n",
    "        svc_c=1.0,\n",
    "    )\n",
    "    start_time = time.time()\n",
    "    svc.train(\n",
    "        data_frames.train[content_col_name],\n",
    "        data_frames.train[label_col_name],\n",
    "    )\n",
    "    end_time = time.time()\n",
    "    \n",
    "    y_hat = svc.predict(data_frames.test[content_col_name])\n",
    "    \n",
    "    mtime = end_time - start_time\n",
    "    acc = accuracy_score(y_hat, data_frames.test[label_col_name])\n",
    "    desc = str(svc)\n",
    "    return [\"SVC\", \"cpu\", mtime, acc, desc, content_col_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d516752-e0f8-451a-9565-5923046afce9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Neural Network Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4fe69b65-c873-42c1-a635-6d8eacf3ec57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c46f635f-ecf7-472a-97bf-dacaea556029",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNClassifier(nn.Module):\n",
    "    def __init__(self, input_size: int, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.input_size: int = input_size\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x) -> None:\n",
    "        return self.model(x)\n",
    "\n",
    "    def info(self) -> str:\n",
    "        description = []\n",
    "        for i, layer in enumerate(self.model):\n",
    "            layer_type = type(layer).__name__\n",
    "            layer_info = str(layer)\n",
    "            description.append(f\"Layer {i}: {layer_type} - {layer_info}\")\n",
    "        return \"\\n\".join(description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "7cfb827b-6c81-400a-8134-68e039a28526",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_nnc(\n",
    "    data_frames: DataFrames,\n",
    "    content_col_name: str,\n",
    "    label_col_name: str,\n",
    "    num_features_tfidf=2000,\n",
    "    num_epochs=2,\n",
    ") -> list:\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    vectorizer = TfidfVectorizer(max_features=num_features_tfidf, stop_words=\"english\")\n",
    "    X = torch.tensor(\n",
    "        vectorizer.fit_transform(data_frames.train[content_col_name]).toarray(),\n",
    "        dtype=torch.float32,\n",
    "    )\n",
    "    y = torch.tensor(\n",
    "        data_frames.train[label_col_name].values, dtype=torch.float32\n",
    "    ).unsqueeze(1)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    X_train = X_train.to(device)\n",
    "    y_train = y_train.to(device)\n",
    "\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "    model = NNClassifier(input_size=num_features_tfidf).to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    acc = 0\n",
    "    total_time = 0.0\n",
    "    for epoch in range(num_epochs):\n",
    "        start_train_epoch = time.time()\n",
    "        model.train()\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        end_train_epoch = time.time()\n",
    "        total_time += end_train_epoch - start_train_epoch\n",
    "    del X_train, y_train\n",
    "\n",
    "    X_test = X_test.to(device)\n",
    "    y_test = y_test.to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_outputs = model(X_test)\n",
    "        val_preds = (val_outputs >= 0.5).float()\n",
    "        accuracy = accuracy_score(y_test.cpu(), val_preds.cpu())\n",
    "        acc = accuracy\n",
    "    \n",
    "    desc = f\"Layers: {model.info()} epochs: {num_epochs}\"\n",
    "    return [\"NNC\", device, total_time, acc, desc, content_col_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0041d5-1c1c-47e9-a4a1-2baaf2cf9245",
   "metadata": {},
   "source": [
    "# TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c55bd3-5258-4e05-9a0b-fe0d419801fa",
   "metadata": {},
   "source": [
    "### Init testing vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "56b88195-087e-429b-9d44-eff90bda4fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROWS = []\n",
    "NUM_OF_TESTS: int = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba9e214-f927-4c64-b8f3-aeeac4840cf2",
   "metadata": {},
   "source": [
    "### LRC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "c63491f2-d141-4456-8790-13f97bc892fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "LRC_ROWS = []\n",
    "for i in range(NUM_OF_TESTS):\n",
    "    LRC_ROWS.append(\n",
    "        run_lrc(\n",
    "            data_frames,\n",
    "            data_frames.content_col_name,\n",
    "            data_frames_clean.label_col_name\n",
    "        )\n",
    "    )\n",
    "for i in range(NUM_OF_TESTS):\n",
    "    LRC_ROWS.append(\n",
    "        run_lrc(\n",
    "            data_frames_1000tok,\n",
    "            data_frames_1000tok.content_clean_col_name_truc,\n",
    "            data_frames_1000tok.label_col_name\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21829c63-e01c-452b-8f4b-290f24deac1e",
   "metadata": {},
   "source": [
    "### SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "affe073c-f7dd-4742-8b76-6c195ccb8c7c",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[107], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m SVC_ROWS \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(NUM_OF_TESTS):\n\u001b[1;32m      3\u001b[0m     SVC_ROWS\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m----> 4\u001b[0m         \u001b[43mrun_svc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdata_frames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdata_frames\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent_col_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdata_frames\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_col_name\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     )\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(NUM_OF_TESTS):\n\u001b[1;32m     11\u001b[0m     SVC_ROWS\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m     12\u001b[0m         run_svc(\n\u001b[1;32m     13\u001b[0m             data_frames_1000tok,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m         )\n\u001b[1;32m     17\u001b[0m     )\n",
      "Cell \u001b[0;32mIn[100], line 15\u001b[0m, in \u001b[0;36mrun_svc\u001b[0;34m(data_frames, content_col_name, label_col_name, num_features_tfidf, ngram_range)\u001b[0m\n\u001b[1;32m      8\u001b[0m svc \u001b[38;5;241m=\u001b[39m SVCClassifier(\n\u001b[1;32m      9\u001b[0m     num_features_tfidf\u001b[38;5;241m=\u001b[39mnum_features_tfidf,\n\u001b[1;32m     10\u001b[0m     ngram_range\u001b[38;5;241m=\u001b[39mngram_range,\n\u001b[1;32m     11\u001b[0m     svc_kernel\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlinear\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     12\u001b[0m     svc_c\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m,\n\u001b[1;32m     13\u001b[0m )\n\u001b[1;32m     14\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 15\u001b[0m \u001b[43msvc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_frames\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcontent_col_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_frames\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlabel_col_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     21\u001b[0m y_hat \u001b[38;5;241m=\u001b[39m svc\u001b[38;5;241m.\u001b[39mpredict(data_frames\u001b[38;5;241m.\u001b[39mtest[content_col_name])\n",
      "Cell \u001b[0;32mIn[57], line 47\u001b[0m, in \u001b[0;36mSVCClassifier.train\u001b[0;34m(self, X_train, y_train)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m, X_train, y_train) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 47\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/programming/fake-news-detector/lib/python3.12/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/programming/fake-news-detector/lib/python3.12/site-packages/sklearn/pipeline.py:469\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Fit the model.\u001b[39;00m\n\u001b[1;32m    427\u001b[0m \n\u001b[1;32m    428\u001b[0m \u001b[38;5;124;03mFit all the transformers one after the other and sequentially transform the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;124;03m    Pipeline with fitted steps.\u001b[39;00m\n\u001b[1;32m    467\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    468\u001b[0m routed_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_method_params(method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m, props\u001b[38;5;241m=\u001b[39mparams)\n\u001b[0;32m--> 469\u001b[0m Xt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    470\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_message(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)):\n\u001b[1;32m    471\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/programming/fake-news-detector/lib/python3.12/site-packages/sklearn/pipeline.py:406\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[0;34m(self, X, y, routed_params)\u001b[0m\n\u001b[1;32m    404\u001b[0m     cloned_transformer \u001b[38;5;241m=\u001b[39m clone(transformer)\n\u001b[1;32m    405\u001b[0m \u001b[38;5;66;03m# Fit or load from cache the current transformer\u001b[39;00m\n\u001b[0;32m--> 406\u001b[0m X, fitted_transformer \u001b[38;5;241m=\u001b[39m \u001b[43mfit_transform_one_cached\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcloned_transformer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessage_clsname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPipeline\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_log_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep_idx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;66;03m# Replace the transformer of the step with the fitted\u001b[39;00m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;66;03m# transformer. This is necessary when loading the transformer\u001b[39;00m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;66;03m# from the cache.\u001b[39;00m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[step_idx] \u001b[38;5;241m=\u001b[39m (name, fitted_transformer)\n",
      "File \u001b[0;32m~/programming/fake-news-detector/lib/python3.12/site-packages/joblib/memory.py:312\u001b[0m, in \u001b[0;36mNotMemorizedFunc.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 312\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/programming/fake-news-detector/lib/python3.12/site-packages/sklearn/pipeline.py:1310\u001b[0m, in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, X, y, weight, message_clsname, message, params)\u001b[0m\n\u001b[1;32m   1308\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[1;32m   1309\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(transformer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_transform\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1310\u001b[0m         res \u001b[38;5;241m=\u001b[39m \u001b[43mtransformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfit_transform\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1311\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1312\u001b[0m         res \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m, {}))\u001b[38;5;241m.\u001b[39mtransform(\n\u001b[1;32m   1313\u001b[0m             X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransform\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\n\u001b[1;32m   1314\u001b[0m         )\n",
      "File \u001b[0;32m~/programming/fake-news-detector/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:2091\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   2084\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params()\n\u001b[1;32m   2085\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf \u001b[38;5;241m=\u001b[39m TfidfTransformer(\n\u001b[1;32m   2086\u001b[0m     norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm,\n\u001b[1;32m   2087\u001b[0m     use_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_idf,\n\u001b[1;32m   2088\u001b[0m     smooth_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msmooth_idf,\n\u001b[1;32m   2089\u001b[0m     sublinear_tf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msublinear_tf,\n\u001b[1;32m   2090\u001b[0m )\n\u001b[0;32m-> 2091\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2092\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[1;32m   2093\u001b[0m \u001b[38;5;66;03m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[1;32m   2094\u001b[0m \u001b[38;5;66;03m# we set copy to False\u001b[39;00m\n",
      "File \u001b[0;32m~/programming/fake-news-detector/lib/python3.12/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/programming/fake-news-detector/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:1372\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1364\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1365\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1366\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1367\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1368\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1369\u001b[0m             )\n\u001b[1;32m   1370\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m-> 1372\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1374\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[1;32m   1375\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/programming/fake-news-detector/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:1259\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1257\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m raw_documents:\n\u001b[1;32m   1258\u001b[0m     feature_counter \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m-> 1259\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m \u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1260\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1261\u001b[0m             feature_idx \u001b[38;5;241m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[0;32m~/programming/fake-news-detector/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:113\u001b[0m, in \u001b[0;36m_analyze\u001b[0;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ngrams \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stop_words \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 113\u001b[0m         doc \u001b[38;5;241m=\u001b[39m \u001b[43mngrams\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop_words\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    115\u001b[0m         doc \u001b[38;5;241m=\u001b[39m ngrams(doc)\n",
      "File \u001b[0;32m~/programming/fake-news-detector/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:246\u001b[0m, in \u001b[0;36m_VectorizerMixin._word_ngrams\u001b[0;34m(self, tokens, stop_words)\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;66;03m# handle stop words\u001b[39;00m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stop_words \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 246\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m [w \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m tokens \u001b[38;5;28;01mif\u001b[39;00m w \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stop_words]\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# handle token n-grams\u001b[39;00m\n\u001b[1;32m    249\u001b[0m min_n, max_n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mngram_range\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "SVC_ROWS = []\n",
    "for i in range(NUM_OF_TESTS):\n",
    "    SVC_ROWS.append(\n",
    "        run_svc(\n",
    "            data_frames,\n",
    "            data_frames.content_col_name,\n",
    "            data_frames.label_col_name\n",
    "        )\n",
    "    )\n",
    "for i in range(NUM_OF_TESTS):\n",
    "    SVC_ROWS.append(\n",
    "        run_svc(\n",
    "            data_frames_1000tok,\n",
    "            data_frames_1000tok.content_clean_col_name_truc,\n",
    "            data_frames_1000tok.label_col_name\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df48e412-6c94-4981-878e-af89e7c455af",
   "metadata": {},
   "source": [
    "### NNC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "a880bdb7-317a-49ce-afdd-1a5f4d08fc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "NNC_ROWS = []\n",
    "for i in range(NUM_OF_TESTS):\n",
    "    NNC_ROWS.append(\n",
    "        run_nnc(\n",
    "            data_frames,\n",
    "            data_frames.content_col_name,\n",
    "            data_frames.label_col_name,\n",
    "            1000,\n",
    "        )\n",
    "    )\n",
    "    torch.cuda.empty_cache()\n",
    "for i in range(NUM_OF_TESTS):\n",
    "    NNC_ROWS.append(\n",
    "        run_nnc(\n",
    "            data_frames_1000tok,\n",
    "            data_frames_1000tok.content_clean_col_name_truc,\n",
    "            data_frames_1000tok.label_col_name,\n",
    "            200,\n",
    "        )\n",
    "    )\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f653118-4ef0-48e4-a595-9e2b689dbcd3",
   "metadata": {},
   "source": [
    "# Results analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "54735d07-b9c5-4f42-a7bf-d077b46d9a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROWS.extend(LRC_ROWS)\n",
    "ROWS.extend(SVC_ROWS)\n",
    "ROWS.extend(NNC_ROWS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "a1a39c2b-4e16-43ca-88bf-95a8448ac07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame(\n",
    "    ROWS,\n",
    "    columns=[\n",
    "        \"Model\",\n",
    "        \"device\",\n",
    "        \"time (seconds)\",\n",
    "        \"accuracy\",\n",
    "        \"additional description\",\n",
    "        \"data type\",\n",
    "    ],\n",
    ")\n",
    "result_df.to_csv(\"results.csv\", sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "e92e919d-b6f7-4acc-b8c9-fc0a0073c793",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>device</th>\n",
       "      <th>time (seconds)</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>additional description</th>\n",
       "      <th>data type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LRC</td>\n",
       "      <td>cpu</td>\n",
       "      <td>13.196943</td>\n",
       "      <td>0.971418</td>\n",
       "      <td>TFIDF: 2000, (1, 2)) L-REG: 1000)</td>\n",
       "      <td>content</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LRC</td>\n",
       "      <td>cpu</td>\n",
       "      <td>13.665311</td>\n",
       "      <td>0.971418</td>\n",
       "      <td>TFIDF: 2000, (1, 2)) L-REG: 1000)</td>\n",
       "      <td>content</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LRC</td>\n",
       "      <td>cpu</td>\n",
       "      <td>13.876855</td>\n",
       "      <td>0.971418</td>\n",
       "      <td>TFIDF: 2000, (1, 2)) L-REG: 1000)</td>\n",
       "      <td>content</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LRC</td>\n",
       "      <td>cpu</td>\n",
       "      <td>3.413579</td>\n",
       "      <td>0.970171</td>\n",
       "      <td>TFIDF: 2000, (1, 2)) L-REG: 1000)</td>\n",
       "      <td>content_clean_truc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LRC</td>\n",
       "      <td>cpu</td>\n",
       "      <td>3.385130</td>\n",
       "      <td>0.970171</td>\n",
       "      <td>TFIDF: 2000, (1, 2)) L-REG: 1000)</td>\n",
       "      <td>content_clean_truc</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Model device  time (seconds)  accuracy             additional description  \\\n",
       "0   LRC    cpu       13.196943  0.971418  TFIDF: 2000, (1, 2)) L-REG: 1000)   \n",
       "1   LRC    cpu       13.665311  0.971418  TFIDF: 2000, (1, 2)) L-REG: 1000)   \n",
       "2   LRC    cpu       13.876855  0.971418  TFIDF: 2000, (1, 2)) L-REG: 1000)   \n",
       "3   LRC    cpu        3.413579  0.970171  TFIDF: 2000, (1, 2)) L-REG: 1000)   \n",
       "4   LRC    cpu        3.385130  0.970171  TFIDF: 2000, (1, 2)) L-REG: 1000)   \n",
       "\n",
       "            data type  \n",
       "0             content  \n",
       "1             content  \n",
       "2             content  \n",
       "3  content_clean_truc  \n",
       "4  content_clean_truc  "
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766f737f-9611-4731-9f2a-036ed5f67684",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Fake News Env)",
   "language": "python",
   "name": "fake-news-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
