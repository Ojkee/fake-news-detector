{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2854a226-b34e-4eef-bba9-099da67b157e",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f02e278c-5b1a-4d4f-86ac-7adf15d5e0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import string\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6d188aa9-4adc-41f8-9585-157077175b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataFrames:\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_file_name: str,\n",
    "        test_file_name: str,\n",
    "        evaluation_file_name: str,\n",
    "    ) -> None:\n",
    "        self.content_col_name: str = \"content\"\n",
    "        self.content_clean_col_name: str = \"content_clean\"\n",
    "        self.content_clean_col_name_no_UNK: str = \"content_clean_no_UNK\"\n",
    "        self.content_clean_col_name_truc: str = \"content_clean_truc\"\n",
    "        self.label_col_name: str = \"label\"\n",
    "        self.train: pd.DataFrame = self.load_data(train_file_name)\n",
    "        self.test: pd.DataFrame = self.load_data(test_file_name)\n",
    "        self.evaluation: pd.DataFrame = self.load_data(evaluation_file_name)\n",
    "\n",
    "    def load_data(self, file_name: str) -> pd.DataFrame:\n",
    "        df = pd.read_csv(f\"dataset/{file_name}.csv\", sep=\";\")\n",
    "        df.dropna(inplace=True)\n",
    "        if not self.content_clean_col_name in df.columns:\n",
    "            df[self.content_col_name] = df[\"title\"] + \" \" + df[\"text\"]\n",
    "            df = df.drop(columns=[\"Unnamed: 0\"])\n",
    "        return df\n",
    "\n",
    "    def get_datasets(self) -> list[pd.DataFrame]:\n",
    "        return [self.train, self.test, self.evaluation]\n",
    "\n",
    "    def get_info(self) -> str:\n",
    "        test_info = self.test.shape\n",
    "        train_info = self.train.shape\n",
    "        evaluation_info = self.evaluation.shape\n",
    "        return f\"DataFrame Shapes:\\n\\tTrain: {train_info}\\n\\tTest: {test_info}\\n\\tEvaluation: {evaluation_info}\\n\"\n",
    "\n",
    "    def save_clean(self, token_limit: int = 10000, num_words_trunc: int = 256) -> None:\n",
    "        # CLEAN\n",
    "        self._init_clean_content([self.train, self.test, self.evaluation])\n",
    "        self.train = self.clean_df(self.train)\n",
    "        self.test = self.clean_df(self.test)\n",
    "        self.evaluation = self.clean_df(self.evaluation)\n",
    "\n",
    "        most_common_words = self.get_most_common_words_counter(\n",
    "            [self.test, self.train],\n",
    "            token_limit,\n",
    "            self.content_clean_col_name,\n",
    "        )\n",
    "        self.train = DataFrames.set_least_common_UNK(\n",
    "            self.train, \"content_clean\", most_common_words\n",
    "        )\n",
    "        self.test = DataFrames.set_least_common_UNK(\n",
    "            self.test, \"content_clean\", most_common_words\n",
    "        )\n",
    "        self.evaluation = DataFrames.set_least_common_UNK(\n",
    "            self.evaluation, \"content_clean\", most_common_words\n",
    "        )\n",
    "        self.train = DataFrames.drop_least_common(\n",
    "            self.train,\n",
    "            self.content_clean_col_name,\n",
    "            self.content_clean_col_name_no_UNK,\n",
    "            most_common_words,\n",
    "        )\n",
    "        self.test = DataFrames.drop_least_common(\n",
    "            self.test,\n",
    "            self.content_clean_col_name,\n",
    "            self.content_clean_col_name_no_UNK,\n",
    "            most_common_words,\n",
    "        )\n",
    "        self.evaluation = DataFrames.drop_least_common(\n",
    "            self.evaluation,\n",
    "            self.content_clean_col_name,\n",
    "            self.content_clean_col_name_no_UNK,\n",
    "            most_common_words,\n",
    "        )\n",
    "        self.train = DataFrames.trunc_text(\n",
    "            self.train,\n",
    "            self.content_clean_col_name_no_UNK,\n",
    "            self.content_clean_col_name_truc,\n",
    "            num_words_trunc,\n",
    "        )\n",
    "        self.test = DataFrames.trunc_text(\n",
    "            self.test,\n",
    "            self.content_clean_col_name_no_UNK,\n",
    "            self.content_clean_col_name_truc,\n",
    "            num_words_trunc,\n",
    "        )\n",
    "        self.evaluation = DataFrames.trunc_text(\n",
    "            self.evaluation,\n",
    "            self.content_clean_col_name_no_UNK,\n",
    "            self.content_clean_col_name_truc,\n",
    "            num_words_trunc,\n",
    "        )\n",
    "        # SAVE\n",
    "        cols_to_save_clean: list[str] = [\n",
    "            self.content_clean_col_name,\n",
    "            self.content_clean_col_name_no_UNK,\n",
    "            self.content_clean_col_name_truc,\n",
    "            self.label_col_name,\n",
    "        ]\n",
    "        self.train.to_csv(\n",
    "            f\"dataset/train_clean.csv\",\n",
    "            sep=\";\",\n",
    "            columns=cols_to_save_clean,\n",
    "        )\n",
    "        self.test.to_csv(\n",
    "            f\"dataset/test_clean.csv\",\n",
    "            sep=\";\",\n",
    "            columns=cols_to_save_clean,\n",
    "        )\n",
    "        self.evaluation.to_csv(\n",
    "            f\"dataset/evaluation_clean.csv\",\n",
    "            sep=\";\",\n",
    "            columns=cols_to_save_clean,\n",
    "        )\n",
    "\n",
    "    def num_unique_words(self, col_name: str) -> int:\n",
    "        result: set = set()\n",
    "        df = pd.concat([self.train, self.test, self.evaluation], ignore_index=True)\n",
    "        df[col_name].str.lower().str.split().apply(result.update)\n",
    "        return len(result)\n",
    "\n",
    "    def get_vocab(self, col_name) -> dict[str, int]:\n",
    "        vectorizer = CountVectorizer()\n",
    "        for df in [self.train, self.test, self.evaluation]:\n",
    "            vectorizer.fit_transform(df[col_name].values)\n",
    "        return vectorizer.vocabulary_\n",
    "\n",
    "    def _init_clean_content(self, dfs: list[pd.DataFrame]) -> list[pd.DataFrame]:\n",
    "        for df in dfs:\n",
    "            df[self.content_clean_col_name] = df[self.content_col_name]\n",
    "        return dfs\n",
    "\n",
    "    def clean_df(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        df = self.to_lower(df, self.content_clean_col_name)\n",
    "        df = self.remove_punctuation(df, self.content_clean_col_name)\n",
    "        df = self.remove_stopword(df, self.content_clean_col_name)\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def to_lower(df: pd.DataFrame, col_name: str) -> pd.DataFrame:\n",
    "        df[col_name] = df[col_name].apply(lambda x: str(x).lower())\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_punctuation(df: pd.DataFrame, col_name: str) -> pd.DataFrame:\n",
    "        re_punctuation = f'[{re.escape(string.punctuation)}\"”“]'\n",
    "        df[col_name] = df[col_name].apply(\n",
    "            lambda x: re.sub(re_punctuation, \" \", str(x))\n",
    "            .lower()\n",
    "            .replace(\"'s\", \"\")\n",
    "            .replace(\"’s\", \"\")\n",
    "        )\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_stopword(df: pd.DataFrame, col_name: str) -> pd.DataFrame:\n",
    "        stop_words = set(stopwords.words(\"english\"))\n",
    "        df[col_name] = df[col_name].apply(\n",
    "            lambda x: \" \".join(\n",
    "                word for word in str(x).split() if not word in stop_words\n",
    "            )\n",
    "        )\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def get_most_common_words_counter(\n",
    "        dfs: list[pd.DataFrame],\n",
    "        token_limit: int,\n",
    "        col_name: str,\n",
    "    ) -> Counter:\n",
    "        word_counter: Counter = Counter()\n",
    "        for df in dfs:\n",
    "            if col_name not in df.columns:\n",
    "                raise ValueError(\"Each DataFrame must have a 'clean_content' column\")\n",
    "            tokens = \" \".join(df[col_name].astype(str)).split()\n",
    "            word_counter.update(tokens)\n",
    "        return Counter(dict(word_counter.most_common(token_limit)))\n",
    "\n",
    "    @staticmethod\n",
    "    def set_least_common_UNK(\n",
    "        df: pd.DataFrame,\n",
    "        col_name: str,\n",
    "        most_common_words: Counter,\n",
    "    ) -> pd.DataFrame:\n",
    "        df[col_name] = df[col_name].apply(\n",
    "            lambda x: \" \".join(\n",
    "                [\n",
    "                    word if word in most_common_words else \"<UNK>\"\n",
    "                    for word in str(x).split()\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def drop_least_common(\n",
    "        df: pd.DataFrame,\n",
    "        col_name: str,\n",
    "        col_name_no_unk: str,\n",
    "        most_common_words: Counter,\n",
    "    ) -> pd.DataFrame:\n",
    "        df[col_name_no_unk] = df[col_name].apply(\n",
    "            lambda x: \" \".join(\n",
    "                [word for word in str(x).split() if word in most_common_words]\n",
    "            )\n",
    "        )\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def trunc_text(\n",
    "        df: pd.DataFrame,\n",
    "        col_name: str,\n",
    "        col_name_trunc: str,\n",
    "        trunc_num: int,\n",
    "    ) -> pd.DataFrame:\n",
    "        df[col_name_trunc] = df[col_name].apply(\n",
    "            lambda x: \" \".join(str(x).split()[:trunc_num])\n",
    "        )\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def label_to_str(label: int) -> str:\n",
    "        return \"Fake\" if label == 1 else \"Not Fake\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "330a54fc-93ef-463f-8e32-b82905934d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frames = DataFrames(\"train\", \"test\", \"evaluation\")\n",
    "data_frames_unique = data_frames.num_unique_words(\"content\")\n",
    "data_frames.save_clean(token_limit=1000)\n",
    "\n",
    "data_frames_clean = DataFrames(\"train_clean\", \"test_clean\", \"evaluation_clean\")\n",
    "data_frames_clean_unique = data_frames_clean.num_unique_words(\"content_clean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb148de-a96e-42bf-9692-97e8a5de6af3",
   "metadata": {},
   "source": [
    "# Logistic Regression Model Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "82984d57-bff7-4fd8-ae13-9f5c6fea3ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c9eda384-57a6-4794-ab71-4fb8f402e608",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionClassifier:\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_features_tfidf: int,\n",
    "        ngram_range: tuple[int, int],\n",
    "        logistic_regression_max_iter: int,\n",
    "    ) -> None:\n",
    "        self.num_features_tfidf = num_features_tfidf\n",
    "        self.ngram_range = ngram_range\n",
    "        self.logistic_regression_max_iter = logistic_regression_max_iter\n",
    "        self.tfidf = TfidfVectorizer(\n",
    "            stop_words=\"english\",\n",
    "            max_features=num_features_tfidf,\n",
    "            ngram_range=ngram_range,\n",
    "        )\n",
    "        self.logistic_regression = LogisticRegression(\n",
    "            max_iter=logistic_regression_max_iter,\n",
    "        )\n",
    "        self.pipeline = Pipeline(\n",
    "            [\n",
    "                (\n",
    "                    \"tfidf\",\n",
    "                    self.tfidf,\n",
    "                ),\n",
    "                (\n",
    "                    \"logistic-regression\",\n",
    "                    self.logistic_regression,\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "        self.y_prediction = ...\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        tfidf_info: str = (\n",
    "            f\"TFIDF: (features: {self.num_features_tfidf}, ngram_range: {self.ngram_range})\"\n",
    "        )\n",
    "        log_reg_info: str = (\n",
    "            f\"LOGISTIC-REGRESSION: (max iter: {self.logistic_regression_max_iter})\"\n",
    "        )\n",
    "        return f\"{tfidf_info}\\n{log_reg_info}\\n\"\n",
    "\n",
    "    def print_info(self) -> None:\n",
    "        print(self)\n",
    "\n",
    "    def train(self, X_train, y_train) -> None:\n",
    "        self.pipeline.fit(X_train, y_train)\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        return self.pipeline.predict(X_test)\n",
    "\n",
    "    def print_report(self, y_test, y_hat) -> None:\n",
    "        report = classification_report(y_test, y_hat, target_names=[\"Not Fake\", \"Fake\"])\n",
    "        print(f\"{self.__class__.__name__} Report:\\n{report}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724a6fc0-dc3a-48dd-b7cd-81615d357a8f",
   "metadata": {},
   "source": [
    "# Test Runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6adab66f-ca18-4fd7-a54d-b2b7ccafe58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_lrc(\n",
    "    data_frames: DataFrames,\n",
    "    content_col_name: str,\n",
    "    label_col_name: str,\n",
    "    num_features_tfidf=2000,\n",
    "    ngram_range=(1, 2),\n",
    "    logistic_regression_max_iter=1000,\n",
    ") -> float:\n",
    "    lgc = LogisticRegressionClassifier(\n",
    "        num_features_tfidf,\n",
    "        ngram_range,\n",
    "        logistic_regression_max_iter,\n",
    "    )\n",
    "    start_time_lgc = time.time()\n",
    "    lgc.train(\n",
    "        data_frames.train[content_col_name],\n",
    "        data_frames.train[label_col_name],\n",
    "    )\n",
    "    end_time_lgc = time.time()\n",
    "    prediction = lgc.predict(data_frames.test[content_col_name])\n",
    "    lgc.print_info()\n",
    "    lgc.print_report(prediction, data_frames.test[label_col_name])\n",
    "\n",
    "    rand_sample = data_frames.evaluation.sample(1)\n",
    "    pred_sample = lgc.predict(rand_sample[content_col_name])\n",
    "    print(rand_sample[content_col_name].values)\n",
    "    label_str_true: str = data_frames.label_to_str(\n",
    "        rand_sample[label_col_name].values[0]\n",
    "    )\n",
    "    label_str_predicted: str = data_frames.label_to_str(pred_sample[0])\n",
    "    print(f\"is:        {label_str_true}\\npredicted: {label_str_predicted}\")\n",
    "    return end_time_lgc - start_time_lgc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e7545845-776f-42eb-b9f8-432a30d94174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTS WITH TIME\n",
    "times: dict[str, float] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3ba59a0f-9c13-4855-8a08-821d8184809c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFIDF: (features: 2000, ngram_range: (1, 2))\n",
      "LOGISTIC-REGRESSION: (max iter: 1000)\n",
      "\n",
      "LogisticRegressionClassifier Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Not Fake       0.97      0.97      0.97      3751\n",
      "        Fake       0.97      0.97      0.97      4366\n",
      "\n",
      "    accuracy                           0.97      8117\n",
      "   macro avg       0.97      0.97      0.97      8117\n",
      "weighted avg       0.97      0.97      0.97      8117\n",
      "\n",
      "['TRAITOR: GOP Senator Lindsey Graham TRASHES and THREATENS President Trump…Tells Voters He Won’t Back Down On AMNESTY PUSH Senator Lindsey Graham made a bold announcement the other day when he told his constituents in South Carolina that if they were against amnesty for illegals and outsourcing, to vote him out. Most Republicans outside of the state of South Carolina have been scratching their heads for years over the popularity of the pro-climate change, war-hawk and pro-amnesty Senator Graham in his home state. It seems like this might be the perfect time for voters in South Carolina to replace the liberal senator who is clearly a traitor to the president and to the Republican Party.Watch Senator Lindsey Graham tell his allies at CNN that President Trump is  weak for  publicly addressing his concerns about his AG Jeff Sessions:Sen. Lindsey Graham says President Trump is demonstrating \"weakness\" through his attacks on AG Jeff Sessions https://t.co/a92rbGyWC1  CNN Politics (@CNNPolitics) July 26, 2017Watch Senator Graham, as he threatens President Trump during an interview with NBC over the possibility that he may fire or  go after  the DOJ s special counsel Robert Mueller, who was hired by  the deputy attorney general, Rod J. Rosenstein to investigate possible ties to Russia and the Trump campaign.WATCH: \"Any effort to go after Mueller could be the beginning of the end of the Trump presidency,\" says Sen. Lindsey Graham. pic.twitter.com/x2scZCKUIo  NBC News (@NBCNews) July 27, 2017Senator Graham has been jealous of Donald Trump since the primaries where Graham got shellacked by Donald Trump. Watch his embarrassing appearance on the Daily Show with Trevor Graham during the primary elections, as he accuses Donald Trump, as well as his own party of being  racists  and  xenophobes .Go to the 3:47 mark to watch Lindsey Graham ask Noah Trevor of The Daily Show if he s a  citizen  or if he has a  green card  to which Trevor replies,  no . Graham then proceeds to tell Noah Trevor that  If I were you, I d be in a hurry. If Trump wins, your days are numbered pal. Young, black, liberal guys from Africa is not gonna work with him. Watch Graham tell his constituents he ll be supporting amnesty whether they agree with him or not. He actually urges South Carolina citizens to vote against him if they disagree with him. He talks about illegal aliens  kids  like their all sweet young children looking for a better life. Someone should tell him about MS-13 the most dangerous international gang in the world that is growing in numbers as Obama opened the flood gates for them to come in while he was president:Watch pro-man made climate change Lindsey Graham attempt to shame President Trump prior to his decision to pull out of the Paris Climate deal:Last but not least, and or anyone who may have forgotten, RINO Lindsey Graham also warned Republicans to back off their attacks on Hillary Clinton and Barack Obama, as Graham warned Republicans risk getting  Burned  on Benghazi issue.']\n",
      "is:        Not Fake\n",
      "predicted: Not Fake\n"
     ]
    }
   ],
   "source": [
    "lrc_time: float = run_lrc(\n",
    "    data_frames,\n",
    "    data_frames.content_col_name,\n",
    "    data_frames_clean.label_col_name\n",
    ")\n",
    "times[\"LRC\"] = lrc_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6a0ab04b-d090-4035-b808-5c82e4606b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFIDF: (features: 2000, ngram_range: (1, 2))\n",
      "LOGISTIC-REGRESSION: (max iter: 1000)\n",
      "\n",
      "LogisticRegressionClassifier Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Not Fake       0.97      0.97      0.97      3754\n",
      "        Fake       0.97      0.97      0.97      4359\n",
      "\n",
      "    accuracy                           0.97      8113\n",
      "   macro avg       0.97      0.97      0.97      8113\n",
      "weighted avg       0.97      0.97      0.97      8113\n",
      "\n",
      "['eu war man man man']\n",
      "is:        Not Fake\n",
      "predicted: Not Fake\n"
     ]
    }
   ],
   "source": [
    "lrc_time_clean: float = run_lrc(\n",
    "    data_frames_clean,\n",
    "    data_frames_clean.content_clean_col_name_truc,\n",
    "    data_frames_clean.label_col_name,\n",
    ")\n",
    "times[\"LRC Clean\"] = lrc_time_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3d0bc835-c060-4b16-a9a9-a3693341eeaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LRC: 17.00977 seconds\n",
      "LRC Clean: 3.18054 seconds\n"
     ]
    }
   ],
   "source": [
    "for fname, ftime in times.items():\n",
    "    print(f\"{fname}: {ftime:.5f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752147a6-486a-47ae-8962-9cd751f0be5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Fake News Env)",
   "language": "python",
   "name": "fake-news-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
